"""
**Module for running a pipeline that takes RNA-seq reads and a GENCODE
annotation and returns information about the length of the 3UTRs and their
polyadenylation patterns.**
"""

from __future__ import division
print('Loading modules ...\n')
import os
import sys
import shutil
import ConfigParser
from multiprocessing import Pool
from multiprocessing import cpu_count
from operator import attrgetter

import re

# only get the debug function if run from Ipython
def run_from_ipython():
    try:
        __IPYTHON__active
        return True
    except NameError:
        return False

if run_from_ipython():
    from IPython.Debugger import Tracer
    #from IPython.core.debugger import Tracer
    debug = Tracer()
else:
    def debug(): pass

from subprocess import Popen
from subprocess import PIPE
import time
import math

# Your own imports
import annotation_parser as genome

class Settings(object):
    """
    An instance of this class holds all the settings parameters obtained from
    the UTR_SETTINGS file. Useful for passing to downstream code.
    """
    def __init__(self, datasets, annotation_path, annotation_format,
                 regions_provided, read_limit, max_cores, chr1, hgfasta_path,
                 polyA, min_utrlen, extendby, cumul_tuning, bigwig,
                 bigwig_datasets, bigwig_savedir, bigwig_url, gem_index,
                 length, annotated_polyA_sites):

        self.get_length = length
        self.datasets = datasets
        self.annotation_path = annotation_path
        self.annotation_format = annotation_format
        self.annotated_polyA_sites = annotated_polyA_sites

        # if annotation is not used, make a list of the region files provided
        if regions_provided:
            self.region_files = [os.path.split(pa)[1] for pa in regions_provided]
            self.regionfile_provided = True
            self.region_dir = os.path.split(regions_provided[0])[0]
        # if not
        else:
            self.region_files = [False]
            self.regionfile_provided = False
            self.region_dir = False

        self.read_limit = read_limit
        self.max_cores = max_cores
        self.chr1 = chr1
        self.hgfasta_path = hgfasta_path
        self.polyA = polyA
        self.min_utrlen = min_utrlen
        self.extendby = int(extendby)
        self.cumul_tuning = cumul_tuning
        self.bigwig = bigwig
        self.bigwig_datasets = bigwig_datasets
        self.bigwig_savedir = bigwig_savedir
        self.bigwig_url = bigwig_url
        self.gem_index = gem_index

        # for debugging only. don't get bed_reads anew but use supplied file.
        self.bed_reads = False

    def DEBUGGING(self):
        """
        For modifying the settings from UTR_FINDER -- only to be used in when
        debugging!
        """

        self.chr1 = True
        #self.chr1 = False
        #self.read_limit = False
        self.read_limit = 1000000 # less than 10000 no reads map
        #self.read_limit = 5000 # less than 10000 no reads map
        self.max_cores = 2
        #self.get_length = False
        self.get_length = True
        self.extendby = 10
        #self.extendby = 100
        self.polyA = True
        #self.polyA = False
        #self.polyA = '/users/rg/jskancke/phdproject/3UTR/the_project/polyA_files'\
                #'/polyA_reads_K562_Whole_Cell_processed_mapped.bed'
        #self.polyA = '/users/rg/jskancke/phdproject/3UTR/the_project/polyA_files'\
                #'/polyA_reads_K562_Cytoplasm_processed_mapped.bed'
        #self.bed_reads = '/users/rg/jskancke/phdproject/3UTR/the_project/temp_files'\
                #'/reads_HeLa-S3_Nucleus.bed'
        self.bigwig = False
        #self.bigwig = True

        if not self.polyA and not self.get_length:
            print('Dude! The program does nothing! Use polyA or length or both!')
            sys.exit()

class Annotation(object):
    """
    An instance of this class has knows where the GENCODE annotation file is. It
    has methods for working with the files that are obtained from the
    annotation.
    """

    def __init__(self, annotation_path, annotation_format, a_polyA_sites_path):
        self.path = annotation_path
        self.ant_format = annotation_format
        self.utrfile_path = ''
        self.feature_coords = ''
        self.a_polyA_sites_path = a_polyA_sites_path
        self.a_polyA_sites_dict = ''

    def get_utrdict(self):
        """
        From the bedfile generated by the annotation, get a dictionary

        :returns: ``utr_dict``
        """
        utr_dict = {}
        for line in open(self.utrfile_path, 'rb'):
            (chrm, beg, end, name, value, strand) = line.split()
            if name in utr_dict:
                print("Duplicate entry! WTF")
                debug()
            utr_dict[name] = (chrm, int(beg), int(end), strand)

        return utr_dict

    def get_annot_polyA_dict(self, chr1):
        """
        Intersect the annotated poly(A) sites with the 3UTR files obtained from
        the annotation. Return a dictionarty of the poly(A) sites that
        intersect.

        :returns: ``polyA_dict``
        """

        polyA_dict = {}

        polyA_path = self.a_polyA_sites_path
        if chr1:
            core, suffix = os.path.splitext(polyA_path)
            polyA_path = os.path.join(core+'_chr1'+ suffix)

        utr_path = self.utrfile_path

        cmd = ['intersectBed', '-wb', '-a', polyA_path, '-b', utr_path]

        # Run the above command -- outside the shell -- and loop through output
        f = Popen(cmd, stdout=PIPE)
        for line in f.stdout:

            (chrm, beg, end, strnd, d, d, d, utr_id, d, d) = line.split()

            if utr_id in polyA_dict:
                polyA_dict[utr_id].append((chrm, beg, end, strnd))
            else:
                polyA_dict[utr_id] = [(chrm, beg, end, strnd)]

        # For the multi-exon utrs, add [] (they will probably not intersect with
        # annotated polyA sites.
        for utr_id in self.feature_coords.iterkeys():
            if utr_id not in polyA_dict:
                polyA_dict[utr_id] = []

        return polyA_dict

    def get_PET_dict(self):
        """
        Intersect the genomic features with PET marks
        """
        PET_file = '/users/rg/jskancke/phdproject/3UTR/PET/all_Pet_merged10+.bed'

        utr_path = self.utrfile_path

        cmd = ['intersectBed', '-wa', '-wb', '-a', PET_file, '-b', utr_path]

        pet_dict = {}
        # Run the above command -- outside the shell -- and loop through output
        f = Popen(cmd, stdout=PIPE)
        for line in f.stdout:

            (chrm, beg, end, d, d, strnd, d, d, d, utr_id, d, d) = line.split()

            if utr_id in pet_dict:
                pet_dict[utr_id].append((chrm, beg, end, strnd))
            else:
                pet_dict[utr_id] = [(chrm, beg, end, strnd)]

        # For the multi-exon utrs, add [] (they will probably not intersect with
        # annotated pet sites.
        for utr_id in self.feature_coords.iterkeys():
            if utr_id not in pet_dict:
                pet_dict[utr_id] = []

        return pet_dict

    def get_cuff_dict(self):
        """
        Intersect the genomic features with cufflinks 3'utr ends
        """
        cuff_file = '/users/rg/jskancke/phdproject/3UTR/CUFF_LINKS/'\
                'cufflinks_3UTR_ends_merged_zeroed.bed'

        region_path = self.utrfile_path

        cmd = ['intersectBed', '-wa', '-wb', '-a', cuff_file, '-b', region_path]

        cuff_dict = {}
        # Run the above command  and loop through output
        f = Popen(cmd, stdout=PIPE)
        for line in f.stdout:

            (chrm, beg, end, d, d, strnd, d, d, d, utr_id, d, d) = line.split()

            if utr_id in cuff_dict:
                cuff_dict[utr_id].append((chrm, beg, end, strnd))
            else:
                cuff_dict[utr_id] = [(chrm, beg, end, strnd)]

        # For the multi-exon utrs, add [] (they will probably not intersect with
        # annotated cuff sites.
        for utr_id in self.feature_coords.iterkeys():
            if utr_id not in cuff_dict:
                cuff_dict[utr_id] = []

        return cuff_dict

class UTR(object):
    """
    Holds all the relevant information about a UTR-object that is going through
    the pipeline. Attributes are either variables that are written to output
    files, or variables that assist in calculating the output variables.

    The UTR object is created from individual exon objects. Often, there is only
    one exon for the UTR object. In these cases, the beg_ext/non_ext variables
    are different, as the beg_ext refers to the extended beg-coordinate.
    However, when a UTR object consists of two or more exons, the internal exons
    have not been extended, so for them the beg/end_ext and beg/end_nonext
    attributes have the same value.
    """

    def __init__(self, chrm, beg, end, strand, val, utr_ID, rpkm, first_covr,
                 polyA_reads, a_polyA_sites, sequence):

        # variables you have to initialize
        self.chrm = chrm

        # ASSUME that the exon has been extended. This might not be the case. 
        self.beg_ext = int(beg)
        self.end_ext = int(end)

        # the total nr of exons and the extendby come in one bundle
        sval, sextendby = val.split('+')
        (self.val, self.extendby) = int(sval), int(sextendby)
        extendby = self.extendby

        self.sequence = sequence

        self.utr_ID = utr_ID
        self.strand = strand
        self.rpkm = rpkm
        self.polyA_reads = polyA_reads
        self.a_polyA_sites = [int(site[1]) for site in a_polyA_sites] # annotated
        self.rel_pos = 'NA' # it might not be updated
        self.epsilon_coord = 'NA'
        self.avrg_coverage = 'NA' # the average coverage -- competing with RPKM!

        # The length of the extended UTR
        self.length_ext = self.end_ext - self.beg_ext

        # Assume not a multi exon
        if self.val == 1:
            self.multi_exon = False
        elif self.val > 1:
            self.multi_exon = True

        # The exon_nr for this exon in the utr.
        self.this_exnr = int(utr_ID.split('_')[-1])

        # Total number of exons in UTR
        self.tot_exnr = self.val

        # initiate the coverage vectors with the first 'coverage'
        self.covr_vector = [int(first_covr)]
        self.cumul_covr = [int(first_covr)]

        # If this exon has been extended, get the non-extended begs and ends. If
        # the exon has not been extended, set them as the same value as the
        # beg.ext. This is confusing, but I'm too tired to sort it out.

        # Start off by assuming no extensions: set non_extended to the same as
        # extended
        self.end_nonext = self.end_ext
        self.beg_nonext = self.beg_ext
        self.length_nonext = self.length_ext

        # If extended, update these values
        if extendby > 0:

            # Get different scenarios for this UTR; depending on which you do
            # different things

            # 1)This UTR has only one exon
            one_exon = (self.this_exnr == self.tot_exnr == 1)

            # 2)This is the 3' exon on the + strand
            fin_ex_plus = (strand == '+') and (self.this_exnr == self.tot_exnr > 1)

            # 3)This is the 3' exon on the - strand
            fin_ex_min = (strand=='-') and (self.tot_exnr>1) and (self.this_exnr==1)

            if one_exon:
                if strand == '+':
                    self.end_nonext = self.end_ext - extendby
                if strand == '-':
                    self.beg_nonext = self.beg_ext + extendby

            if fin_ex_plus:
                self.end_nonext = self.end_ext - extendby

            if fin_ex_min:
                self.beg_nonext = self.beg_ext + extendby

            # if either of the cases were encountered, update the unextended
            # length
            if one_exon or fin_ex_plus or fin_ex_min:
                self.length_nonext = self.end_nonext - self.beg_nonext

        # if multi exon, make a beg-end list (extended and nonext) for the
        # utrs
        if self.multi_exon:
            self.begends_ext = [(self.beg_ext, self.end_ext)]
            self.begends_nonext = [(self.beg_nonext, self.end_nonext)]

    def __repr__(self):
        return self.utr_ID[-8:]

    def __str__(self):
        return "\nChrm\t{0}\nBeg\t{1}\nEnd\t{2}\nStrand\t{3}\n"\
                .format(self.chrm, self.beg_nonext, self.end_nonext, self.strand)

    def is_empty(self):
        """
        Return ``True`` if UTR object has no coverage, otherwise ``False``.
        """

        if self.strand == '-':
            covr_slice = iter(self.covr_vector[self.extendby:])
        elif self.strand == '+':
            covr_slice = iter(self.covr_vector[:-self.extendby])

        # Check if covr_vector is non-empty in non-extended 3UTR
        for val in covr_slice:
            if val > 0:
                return False

        return True

    def update_utr(self, new_exon, total_nr_reads):
        """
        For multi-exon 3UTRs. Updates the attributes of the UTR-object with the
        next exon with the next exon. The first exon's ID will be the one
        written to file.
        """

        # Update lengths
        self.length_nonext += new_exon.length_nonext
        self.length_ext += new_exon.length_ext

        # Update RPKM with weights on the lenghts of the two exons You need to
        # recover the number of reads falling over the UTRS 
        R_self = (self.rpkm)*(total_nr_reads)*(self.length_nonext)/(10**9)
        R_new = (new_exon.rpkm)*(total_nr_reads)*(new_exon.length_nonext)/(10**9)

        # Update rpkm with these two values and the new length
        self.rpkm = ((10**9)*(R_self+R_new))/(total_nr_reads*self.length_nonext)

        # Update begends lists to know where all exons begin and end
        self.begends_ext.append((new_exon.beg_ext, new_exon.end_ext))
        self.begends_nonext.append((new_exon.beg_nonext, new_exon.end_nonext))

        # Update polyA_reads if any
        for side in ('plus_strand', 'minus_strand'):
            if new_exon.polyA_reads[side][0] != []:
                # NOTE you're not sure if the below code works as you think.
                # this is because of the format you put the polyA reads into is
                # different if you have reads there or not ...

                avrges = new_exon.polyA_reads[side][0]
                supprt = new_exon.polyA_reads[side][1]
                for (pA_site, pA_support) in zip(avrges, supprt):
                    self.polyA_reads[side][0].append(pA_site)
                    self.polyA_reads[side][1].append(pA_support)

        # Update annotated polyA sites
        for site in new_exon.a_polyA_sites:
            self.a_polyA_sites.append(site)

        # Update sequence and coverage vectors
        # This works, because the utrs have been sorted by chrm_beg. The next
        # exon always has HIGHER utr_beg coordinate. You don't need to
        # special-case for each strand for 'covr_vector', since this array is
        # strand-inspecific.

        # No covr_vector is strand-insensitive
        self.covr_vector = self.covr_vector + new_exon.covr_vector

        # If last exon, recalculate the cumulative coverage
        if new_exon.this_exnr == self.tot_exnr:
            self.cumul_covr = [sum(self.covr_vector[:x])
                               for x in range(1, self.length_ext+1)]

def frmt(element):
    """
    Return floats as strings with four decimals.
    Return ints as strings.
    Return all other objects as they came in.
    """

    if type(element) is float:
        return format(element, '.4f')
    if type(element) is int:
        return str(element)
    else:
        return element

class FullLength(object):
    """
    Class for writing the 'length' file. Calculates the output based on
    variables from a UTR instance. Also writes the header of the 'length' output
    file.
    """

    def __init__(self, utr_ID):
        self.utr_ID = utr_ID

        self.PAS_dist = 'NA'
        self.has_PAS = 'NA'
        self.eps_dstream_coverage = 'NA'
        self.eps_ustream_coverage = 'NA'
        self.cuml_rel_size = 'NA'
        self.annotTTS_dist = 'NA'

        self.epsilon = 0.995

    def header_dict(self, this_utr):
        """
        Return a dictionary that maps header-entries to the UTR or FullLength
        instances that are being written to file. This ensures that each column
        contains the data that corresponds to the colum header.

        Note that you are writing the beg/end of the extended 3UTRs.
        """
        return dict((
                    ('chrm', this_utr.chrm),
                    ('beg', frmt(this_utr.beg_ext)),
                    ('end', frmt(this_utr.end_ext)),
                    ('utr_ID', frmt(this_utr.utr_ID[:-2])),
                    ('strand', this_utr.strand),
                    ('3utr_extended_by', frmt(this_utr.extendby)),
                    ('epsilon_coord', frmt(this_utr.epsilon_coord)),
                    ('epsilon_rel_size', frmt(self.cuml_rel_size)),
                    ('epsilon_downstream_covrg', frmt(self.eps_dstream_coverage)),
                    ('epsilon_upstream_covrg', frmt(self.eps_ustream_coverage)),
                    ('annotation_distance', frmt(self.annotTTS_dist)),
                    ('epsilon_PAS_type', self.has_PAS),
                    ('epsilon_PAS_distance', frmt(self.PAS_dist)),
                    ('3utr_RPKM', frmt(this_utr.rpkm)),
                    ('3utr_average_coverage', frmt(this_utr.avrg_coverage))
                     ))

    def header_order(self):
        """
        Return a string with the titles of the columns in the 'length' output
        file. The order in which titles appear here is also the order in which
        they appear in the output file.
        """
        return """
        chrm
        beg
        end
        3utr_extended_by
        strand
        utr_ID
        epsilon_coord
        epsilon_rel_size
        epsilon_downstream_covrg
        epsilon_upstream_covrg
        annotation_distance
        epsilon_PAS_type
        epsilon_PAS_distance
        3utr_RPKM
        3utr_average_coverage
        """.split()

    def write_header(self, outfile):
        """
        Write the header from :func:`header_order` to *outfile*.
        """
        outfile.write('\t'.join(self.header_order()) + '\n')

    def calculate_output(self, pas_patterns, this_utr):
        """
        Calculate output variables if *this_utr* is nonempty:
            1) Get cumulative coverage using the *epsilon* parameter
            2) See if there is a PAS close to the estimated end, and if so,
            return the distance to that PAS
        """
        # Don't do anything if coverage vector is empty
        if this_utr.is_empty():
            return

        # Call different strand-specific functions
        if this_utr.strand == '-':
            # calculate the cumulative values
            self.cumul_minus(this_utr)
            # Get the average coverage of the utr
            ext = this_utr.extendby
            non_ext_covr = this_utr.covr_vector[ext:]
            this_utr.avrg_coverage = sum(non_ext_covr)/len(non_ext_covr)

        if this_utr.strand == '+':
            # calculate the cumulative values
            self.cumul_plus(this_utr)
            # Get the average coverage of the utr
            ext = this_utr.extendby
            non_ext_covr = this_utr.covr_vector[:-ext]
            this_utr.avrg_coverage = sum(non_ext_covr)/len(non_ext_covr)

        # calculate the PAS and pas distance for 'length'
        self.get_pas_epsilon(pas_patterns, this_utr)

        # See if there is an annotated TTS end close to the length-defined end
        self.annotTTS_dist = self.annot_TTS_dist(this_utr)

    def annot_TTS_dist(self, this_utr):
        """
        Check if the 3UTR length end has an annotated TTS nearby
        """
        eps_end = this_utr.epsilon_coord
        for apoint in this_utr.a_polyA_sites:
            if eps_end-50 < apoint < eps_end+50:
                return eps_end - apoint

        # If you make it here, a close-by aTTS site has not been found
        return 'NA'

    def cumul_minus(self, this_utr):
        """
        Calculate the position of cumulative coverage (e.g 0.98) relative to the
        length of the annotated 3UTR. This is refered to as the *epsilon*
        position. Calculate the coverage on both sides of this position.

        This is the new version, where I calculate the epsilon coverage
        including the extension.
        """

        covr_vector = this_utr.covr_vector
        cumul_covr = this_utr.cumul_covr

        # Get normalized cuml-coverage of non-extended 3UTR. save to this_utr
        covr_sum = sum(covr_vector)
        this_utr.norm_cuml = [1-val/covr_sum for val in cumul_covr]

        # Get the extended-utr-relative position where 99.5% of the reads
        # have landed

        # Assume rel_pos is 0 and then update it
        rel_pos = 0

        for ind, el in enumerate(this_utr.norm_cuml):
            # What if el is never < than self.epsilon?
            if el < self.epsilon:
                rel_pos = ind
                length_ext = this_utr.length_ext
                self.cuml_rel_size = (length_ext-rel_pos)/this_utr.length_nonext
                break

        # Save relative cumulative position with the this utr for later usage
        this_utr.rel_pos = rel_pos

        # Update the epsilon coordinate relative to the genome
        this_utr.epsilon_coord = this_utr.beg_ext + this_utr.rel_pos

        # Calculate the mean coverage on both sides of rel_pos
        self.eps_dstream_coverage = sum(covr_vector[rel_pos-50:rel_pos])/50
        self.eps_ustream_coverage = sum(covr_vector[rel_pos:rel_pos+50])/50

        if rel_pos < 50 and rel_pos!= 0:
            self.eps_dstream_coverage = sum(covr_vector[:rel_pos])/rel_pos

        if rel_pos == 0:
            self.eps_dstream_coverage = 0

    def cumul_plus(self, this_utr):
        """
        See cumul_minus
        """
        covr_vector = this_utr.covr_vector
        cumul_covr = this_utr.cumul_covr # extended cumul_covr

        # Get normalized cuml-coverage of EXTENDED 3UTR
        covr_sum = sum(covr_vector) # extended sum(covr_vector)
        this_utr.norm_cuml = [val/covr_sum for val in cumul_covr]

        # Assume rel_pos is 0 and then update it
        rel_pos = 0

        # Get the point where epsilon percent of reads have landed (e.g. 99.5)
        for ind, el in enumerate(reversed(this_utr.norm_cuml)):
            if el < self.epsilon:
                length_ext = this_utr.length_ext
                rel_pos = length_ext - ind
                self.cuml_rel_size = rel_pos/float(this_utr.length_nonext)
                break

        # Save relative position (relative to extended 3utr) with the object
        this_utr.rel_pos = rel_pos

        # Update the epsilon coordinate relative to the genome
        this_utr.epsilon_coord = this_utr.beg_ext + this_utr.rel_pos

        # Calculate the mean coverage on both sides of epsilon_pos.
        self.eps_dstream_coverage = sum(covr_vector[rel_pos:rel_pos+50])/float(50)
        self.eps_ustream_coverage = sum(covr_vector[rel_pos-50:rel_pos])/float(50)

        # Check for the occasions when rel_pos -50 is less than 0
        if rel_pos < 50 and rel_pos != 0:
            self.eps_ustream_coverage = sum(covr_vector[:rel_pos])/float(rel_pos)

        if rel_pos == 0:
            self.eps_ustream_coverage = 0

    def get_pas_epsilon(self, pas_patterns, this_utr):
        #"""
        #Return all close-by PAS and their distances.
        #"""

        if this_utr.strand == '+':
            rel_pos = this_utr.rel_pos
            # For negative strand, take the length of the sequence mins rel_pos.
            # (because the sequence is in 5->3' direction

        if this_utr.strand == '-':
            rel_pos = len(this_utr.sequence) - this_utr.rel_pos

        temp_PAS = []
        pas_seq = this_utr.sequence[rel_pos-40:rel_pos]
        for pas_exp in pas_patterns:
            temp_PAS.append([(m.group(), 40-m.end()) for m in
                            pas_exp.finditer(pas_seq)])

        if sum(temp_PAS, []) != []:
            has_PAS = sum(temp_PAS, [])
            if len(has_PAS) == 1:
                (has_PAS, PAS_dist) = ([has_PAS[0][0]], [has_PAS[0][1]])
            else:
                (has_PAS, PAS_dist) = zip(*has_PAS)

            self.has_PAS = '#'.join([str(pas) for pas in has_PAS])
            self.PAS_dist = '#'.join([str(dist) for dist in PAS_dist])

    def write_output(self, outobject, this_utr):
        """
        Write out the output as determined in 'header_order()'
        """

        # Get output dict
        output_dict = self.header_dict(this_utr)

        # Write output that corresponds to header in self.header_order
        output = [output_dict[hedr] for hedr in self.header_order()]

        outobject.write('\t'.join(output) + '\n')

class PolyAReads(object):
    """
    Returns object for writing to 'polyA' file. Contains method for writing
    header of 'polyA' file.
    """

    def __init__(self, utr_ID):
        self.utr_ID = utr_ID

        # variables to be printed
        self.polyRead_sites = 'NA'
        self.rel_polyRead_sites = 'NA'
        self.ds_covrg = 'NA'
        self.us_covrg = 'NA'
        self.annotation_support = 'NA'
        self.all_PAS = 'NA'
        self.number_supporting_reads = 'NA'

    def write_header(self, outfile):
        """
        Write the header of the 'polyA' output file
        """
        outfile.write('\t'.join(self.header_order()) + '\n')

    def header_dict(self, this_utr, polA_nr, pAcoord, nr_supp_pA, ds_covr,
                    us_covr, annotpA_dist, nearbyPAS, PAS_dist):
        """
        See the equivalent method for the 'FullLength' class.
        """

        return dict((
                    ('chrm', this_utr.chrm),
                    ('beg', frmt(this_utr.beg_ext)),
                    ('end', frmt(this_utr.end_ext)),
                    ('utr_ID', this_utr.utr_ID[:-2]),
                    ('polyA_number', frmt(polA_nr)),
                    ('strand', this_utr.strand),
                    ('polyA_coordinate', frmt(pAcoord)),
                    ('polyA_coordinate_strand', self.pAcoord_strand),
                    ('number_supporting_reads', frmt(nr_supp_pA)),
                    ('coverage_50nt_downstream', frmt(ds_covr)),
                    ('coverage_50nt_upstream', frmt(us_covr)),
                    ('annotated_polyA_distance', frmt(annotpA_dist)),
                    ('nearby_PAS', nearbyPAS),
                    ('PAS_distance', frmt(PAS_dist)),
                    ('3utr_RPKM', frmt(this_utr.rpkm))
                    ))

    def header_order(self):
        """
        See the equivalent method for the 'FullLength' class.
        """
        return """
        chrm
        beg
        end
        utr_ID
        polyA_number
        strand
        polyA_coordinate
        polyA_coordinate_strand
        number_supporting_reads
        coverage_50nt_downstream
        coverage_50nt_upstream
        annotated_polyA_distance
        nearby_PAS
        PAS_distance
        3utr_RPKM
        """.split()

    def writeA_output(self, outobject, this_utr, side):
        """
        Write the output in the order determined in 'header_order()'. For each
        UTR, there might be several polyA clusters. Each cluster gets a line in
        the output file.
        """

        if side == 'plus_strand':
            self.pAcoord_strand = '+'
        else:
            self.pAcoord_strand = '-'

        # The column-order in which the output should be printed
        output_order = self.header_order()

        # Write one output line for each polyA cluster
        for indx, site in enumerate(self.polyRead_sites):
            polAnr = indx + 1
            rel_polyA_site = self.rel_polyRead_sites[indx]
            nr_supp_pA = len(this_utr.polyA_reads[side][1][indx])

            ds_covrg = self.ds_covrg[indx]
            us_covrg = self.us_covrg[indx]
            annotpA_dist = self.annotation_support[indx]

            # One site might have several PAS sites
            (nearbyPAS, PAS_dist) = self.all_PAS[rel_polyA_site]

            # If found, Turn the PAS sites and their dists into strings
            if (nearbyPAS, PAS_dist) != ('NA', 'NA'):
                nearbyPAS = '#'.join([str(pas) for pas in nearbyPAS])
                PAS_dist = '#'.join([str(dist) for dist in PAS_dist])

            # Get the output dictionary with updated values
            output_dict = self.header_dict(this_utr, polAnr, site, nr_supp_pA,
                                           ds_covrg, us_covrg, annotpA_dist,
                                           nearbyPAS, PAS_dist)

            output = [output_dict[hedr] for hedr in output_order]

            outobject.write('\t'.join(output) + '\n')

    def calculateA_output(self, pas_patterns, this_utr, side):
        """
        Calculate the following output:
            # The relative position of the cluster to non-extended UTR beg
            # Average coverage 50nt on both sides of the cluster
            # The distance to an annotated polyA site, if distance is less than
              40 nt.
            # The distance and type of PAS, if closer than 40 nt.
        """
        self.polyRead_sites = this_utr.polyA_reads[side][0]
        # Get the relative to EXTENDED utr(!) location of the polyA sites,
        # because it's from the extended utr we take the coverage values.
        self.rel_polyRead_sites = [pos-this_utr.beg_ext for pos in
                                       self.polyRead_sites]

        # Determine coverage relative to the vector
        left_covrg = []
        right_covrg = []

        for p in self.rel_polyRead_sites:
            right_covrg.append(sum(this_utr.covr_vector[p:p+50])/float(50))

            # Left-covrg depends on if you are close to the beg of cover vector
            if p < 50 and p != 0:
                left_covrg.append(sum(this_utr.covr_vector[:p])/float(p))
            elif p == 0:
                left_covrg.append(0)
            else:
                left_covrg.append(sum(this_utr.covr_vector[p-50:p])/float(50))

        # Find upstream/downstream from left/right depending on strand
        if side == 'plus_strand':
            self.ds_covrg = right_covrg
            self.us_covrg = left_covrg

        if side == 'minus_strand':
            self.ds_covrg = left_covrg
            self.us_covrg = right_covrg

        #2) Is there an annotated polyA site nearby?
        # Report if there is one within +/- 40 nt and report the distance. Also
        # report if no distance is found. the list of found/notfound corresponds
        # to the elements in self.polyRead_sites
        self.annotation_support = self.read_annot_support(this_utr.a_polyA_sites,
                                                          side)

        #3) If there is a PAS nearby? all_PAS hold all the PAS and their
        #distances for all the polyA read clusters in the 3UTR
        self.all_PAS = self.get_pas(pas_patterns, this_utr.sequence, side)

    def get_pas(self, pas_patterns, sequence, side):
        """
        go through the -40 from the polya read average. collect pas and distance
        as you find them. if side is negative, get the reverse complement
        """

        all_pas = {}
        for rpoint in self.rel_polyRead_sites:
            if side == 'plus_strand':
                pas_seq = sequence[rpoint-40:rpoint]

                # special case if the polya read is is early in the sequence
                if rpoint < 40:
                    pas_seq = sequence[:rpoint]

            # if negative strand, take the reverse complement 
            if side == 'minus_strand':
                pas_seq = sequence[rpoint:rpoint+40]
                pas_seq = reverseComplement(pas_seq)

            temp_pas = []
            for pas_exp in pas_patterns:
                temp_pas.append([(m.group(), m.start()) for m in
                                pas_exp.finditer(pas_seq)])

            if sum(temp_pas, []) != []:
                has_pas = sum(temp_pas, [])
                if len(has_pas) == 1:
                    (has_pas, pas_dist) = ([has_pas[0][0]], [has_pas[0][1]])
                else:
                    (has_pas, pas_dist) = zip(*has_pas)

                all_pas[rpoint] = (has_pas, pas_dist)

            # in case no pas are found, return 'NA'
            else:
                all_pas[rpoint] = ('NA', 'NA')

        return all_pas

    def read_annot_support(self, annotated_polyA_sites, side):
        """
        Check if the polyA cluster has an annotated polyA site nearby.
        """
        supp = []
        for rpoint in self.polyRead_sites:
            found = False
            for apoint in annotated_polyA_sites:
                if rpoint-40 < apoint < rpoint+40:
                    found = True
                    if side == 'plus_strand':
                        found_distance = rpoint-apoint + 1
                    else:
                        found_distance = rpoint-apoint
                    # RESULT: for +, distance is one too little
                    # Otherwise, it seems good.
                    break
            if found:
                supp.append(found_distance)
            if not found:
                supp.append('NA')

        return supp

def PETorNot(rpoint, pet_sites, strand):
    """
    Within 100 of a pet or not?
    """
    for pet in pet_sites:
        if int(pet[1])-100 < rpoint < int(pet[2])+100:
            return 1

    return 0

def Cuffme(rpoint, cuff_sites, strand):
    """
    Within 1 of cufflinks 3UTR end or not? The cufflink ends are already +/- 50,
    now you look +/- 50, so in total you look 100nt around the cufflinksend.
    """
    for cuff in cuff_sites:
        if int(cuff[1])-50 < rpoint < int(cuff[2])+50:
            return 1

    return 0

def annotation_dist(rpoint, annotated_polyA_sites, strand):
    """
    Check if the polyA cluster has an annotated polyA site nearby. If it has,
    return the distance to the site. If it doesn't, return 'NA'.
    """

    for a_pA in annotated_polyA_sites:
        apoint = int(a_pA[1])
        if apoint-40 < rpoint < apoint+40:
            if strand == '+':
                return rpoint-apoint + 1
            else:
                return rpoint-apoint
            # RESULT: for +, distance is one too little
            # Otherwise, it seems good.
            break

    # If you get here, you didn't find any annotated distances
    return 'NA'

def get_pas_and_distance(pas_patterns, sequence):
    """
    Go through the -40 from the polya read average. Collect PAS and distance
    as you find them. Must supply poly(A) sites relative to UTR-beg. The
    sequence must also be relative to UTR-beg (3->5 direction)
    """

    pases = []
    temp_pas = []

    for pas_exp in pas_patterns:
        temp_pas.append([(m.group(), m.start()) for m in
                        pas_exp.finditer(sequence)])

    if sum(temp_pas, []) != []:
        has_pas = sum(temp_pas, [])
        if len(has_pas) == 1:
            (has_pas, pas_dist) = ((has_pas[0][0],), (has_pas[0][1],))
        else:
            (has_pas, pas_dist) = zip(*has_pas)

        #(('GATAAA', 'AATGAA'), (22, 28))
        # This is what you return.
        pases.append((has_pas, pas_dist))

    # in case no pas are found, return 'na'
    else:
        # If both were 'NA', return it
        pases.append(('NA', 'NA'))

    # Join the pases together. if the length is 1, it means that both are
    # 'NA', 'NA'. Just return one of them.
    if len(set(pases)) == 1:
        return(pases[0])

    # If not, join the two together. A zip perhaps? If one of them is 'NA',
    # remove it, and return the other
    else:
        try:
            pases.remove(('NA', 'NA'))
            return pases[0]
        except ValueError:
            zipases = zip(*pases)
            return (sum(zipases[0], ()), sum(zipases[1], ()))

def reverseComplement(sequence):
    complement = {'A':'T','C':'G','G':'C','T':'A','N':'N'}
    return "".join([complement[nt] for nt in sequence[::-1]])

def zcat_wrapper(dset_id, bed_reads, read_limit, out_path, polyA, polyA_path,
                 get_length):
    """
    Wrapper around zcat. Call on gem-mapped reads. Write uniquely mapped
    reads (up to 2 mismatches) to .bed file.

    If polyA parameter was passed as True, write unmapped reads with leading
    poly(T) or tailing poly(A) to .bed file. The read is written to the bed-file
    as stripped of polyA or polyT stretch. Either 5 contiguous A/Ts or 6 A/Ts in
    the last/first 7 nucleotides must be present for the read to be considered
    as a putative poly(A) read.
    """

    # Keep track on the number of reads you get for the sake of RPKM
    total_reads = 0
    total_mapped_reads = 0
    acount = 0
    tcount = 0

    # TODO If the out_path exists, print to screen that you're using the version in
    # the temp-dir. Continue with the get_length flag as false. Note: this will
    # disable speed-runs, where you want to do something quick. Why did I want
    # to do it again? I don't remember.
    #debug()

    # if get_length is false AND! polyA is given, return nothing
    if not get_length and polyA != True:
        return total_mapped_reads, total_reads, acount, tcount

    # File objects for writing to
    out_file = open(out_path, 'wb')
    polyA_file = open(polyA_path, 'wb')

    # Accept up to two mismatches. Make as set for speedup.
    acceptable = set(('1:0:0', '0:1:0', '0:0:1'))

    # A dictionary of strands
    getstrand = {'R':'-', 'F':'+'}

    # Run zcat with -f to act as noram cat if the gem-file is not compressed
    cmd = ['zcat', '-f'] + bed_reads
    f = Popen(cmd, stdout=PIPE)

    # You need an overall method for determining tails.
    # Two pieces of information
    # 1) minimum length of tail
    # 2) nr of non-A/T nucleotides ratio 1:3, 1:4, 1:5?
    # 4) Commence with a minimum UTR length of 4 and 1:5 ratio
    # 5) let these be optional in the final version of Utail
    # 7) ratio denominator must b
    # 8) also accept and iteratively trim 2/10 2/12!

    min_length = 5
    ratio_denominator = 6
    rd = ratio_denominator

    min_A = 'A'*min_length
    min_T = 'T'*min_length

    # Make regular expression for getting read-start
    start_re = re.compile('[0-9]*')
    trail_A = re.compile('A{{{0},}}'.format(min_length))
    lead_T = re.compile('T{{{0},}}'.format(min_length))
    non_A = re.compile('[^A]')
    non_T = re.compile('[^T]')

    for map_line in f.stdout:
        #
        try:
            (ID, seq, quality, mapinfo, position) = map_line.split('\t')
        except ValueError:
            print 'read error from line {0} in {1} '.format(map_line, bed_reads)
            continue

        total_reads +=1

        # If short, only get up to 'limit' of reads
        if read_limit and total_reads > read_limit:
            break

        # Acceptable and poly(A) reads are mutually exclusive.
        if mapinfo in acceptable and get_length == True:
            # Get chromosome
            chrom, rest = position.split(':')
            # Get the strand
            strand = getstrand[rest[0]]
            # Get read beg
            beg = start_re.match(rest[1:]).group()

            # Write to file
            out_file.write('\t'.join([chrom, beg, str(int(beg)+len(seq)), '0',
                                      '0', strand]) + '\n')
            total_mapped_reads +=1

        # When looking for poly(A) reads, filter by non-uniquely mapped reads
        if polyA == True:
            if mapinfo[:5] == '0:0:0':
                if seq[:2] == 'NN':
                    seq = seq[2:]

                # If more than two ambigious, discard.
                if seq.count('N') > 2:
                    continue

                # Check for putative poly(T)-head. Remove tail and write to file.
                if seq[:min_length] == min_T or\
                   seq[:rd].count('T') >=rd-1 or\
                   seq[:2*rd].count('T') >=2*rd-2:

                    t_strip = strip_tailT(seq, lead_T, non_T, min_length, rd,
                                          min_T)
                    striplen = len(t_strip)

                    if striplen > 25:
                        seqlen = len(seq)
                        tail = seq[:seqlen-striplen]
                        name = '#'.join([t_strip, tail, 'T'])

                        polyA_file.write(name + '\n')
                        tcount +=1

                # Check for putative poly(A)-tail. Remove tail and write to file.
                if seq[-min_length:] == min_A or\
                   seq[-rd:].count('A') >=rd-1 or\
                   seq[-2*rd:].count('A') >=2*rd-2:

                    a_strip = strip_tailA(seq, trail_A, non_A, min_length, rd,
                                          min_A)
                    striplen = len(a_strip)

                    if striplen > 25:
                        seqlen = len(seq)
                        tail = seq[-(seqlen-striplen):]

                        name = '#'.join([a_strip, tail, 'A'])

                        polyA_file.write(name + '\n')
                        acount += 1

    out_file.close()
    polyA_file.close()

    print('\n{0}: Nr. of total reads: {1}'.format(dset_id, total_reads))
    print('{0}: Nr. of readsReads with poly(A)-tail: {1}'.format(dset_id, acount))
    print('{0}: Nr. of readsReads with poly(T)-tail: {1}\n'.format(dset_id, tcount))

    return (total_mapped_reads, total_reads, acount, tcount)

def strip_tailA(seq, trail_A, non_A, min_length, rd, min_A):
    """
    Strip the polyA tail of a read iteratively. Strip things ending in a stretch
    of As and stript 1/min_length and 2/min_length*2
    """

    if seq[-min_length:] == min_A:
        # Remove all trailing As on reverse sequence; then reverse
        # again. rexexp only works from the beginning
        seq = strip_tailA(trail_A.sub('', seq[::-1], count=1)[::-1], trail_A,
                           non_A, min_length, rd, min_A)

    if seq[-rd:].count('A') >=rd-1:
        # First remove the non-A character
        # Then remove all trailing As
        seq = strip_tailA(trail_A.sub('', non_A.sub('', seq[::-1], count=1),
                           count=1)[::-1], trail_A, non_A, min_length, rd, min_A)

    if seq[-2*rd:].count('A') >=2*rd-2:
        # First remove the non-A characters
        # Then remove all trailing As
        seq = strip_tailA(trail_A.sub('', non_A.sub('', seq[::-1], count=2),
                           count=1)[::-1], trail_A, non_A, min_length, rd, min_A)

    return seq

def strip_tailT(seq, lead_T, non_T, min_length, rd, min_T):
    """
    Strip the polyT tail of a read iteratively.
    """

    if seq[:min_length] == min_T:
        # Remove all leading Ts
        seq = strip_tailT(lead_T.sub('', seq, count=1), lead_T, non_T,
                          min_length, rd, min_T)

    if seq[:rd].count('T') >=rd-1:
        # First remove the non-T character
        # Then remove all leading Ts
        seq = strip_tailT(lead_T.sub('', non_T.sub('', seq, count=1), count=1),
                           lead_T, non_T, min_length, rd, min_T)

    if seq[:2*rd].count('T') >=2*rd-2:
        # First remove the non-T characters
        seq = strip_tailT(lead_T.sub('', non_T.sub('', seq, count=2), count=1),
                           lead_T, non_T, min_length, rd, min_T)
        # Then remove all leading Ts
    return seq

def coverage_wrapper(dset_id, filtered_reads, utrfile_path):
    """
    Wrapper around coverageBed. Calcuates coverage of the reads over the
    3UTR-regions, or whatever region was sent in to the program. If the region
    is large, this takes a lot of time, and the resulting file is huge.
    """

    ## XXX This is a sin: I'm introducing a once-in-a-lifetime piece of code
    #here. It outputs the coverage 

    # Rename path to 'covered_...'
    (dirpath, basename) = os.path.split(filtered_reads)
    out_path = os.path.join(dirpath, 'covered_'+dset_id)
    outfile = open(out_path, 'wb')

    cmd = ['coverageBed', '-d', '-a', filtered_reads, '-b', utrfile_path]

    f = Popen(cmd, stdout=outfile)
    f.wait()

    outfile.close()

    return out_path

def join_multiexon_utr(multi_exon_utr, total_nr_reads):
    """
    For multi-exon UTRs. When all exons in an UTR has been accounted for, start
    with the first exon (determined by sorting), and call
    'first_exon.update_utr(new_exon)' for all the new exons.
    """
    # Sort utr-exon instances according to utr-beg
    multi_exon_utr.sort(key = attrgetter('beg_nonext'))
    # Select the first exon in the utr as the 'main' utr. Add information from
    # the other utrs to this utr.
    main_utr = multi_exon_utr[0]
    for new_exon in multi_exon_utr[1:]:
        main_utr.update_utr(new_exon, total_nr_reads)

    return main_utr

def output_writer(dset_id, coverage, annotation, rpkm, polyA_reads, settings,
                  total_nr_reads, output_dir, polyA, utr_seqs):
    """
    Putting together all the info on the 3UTRs and writing to files. Write
    one file mainly about the length of the 3UTR, and write another file about
    the polyA sites found in the 3UTR.
    """

    a_polyA_sites_dict = annotation.a_polyA_sites_dict
    feature_coords = annotation.feature_coords

    (dirpath, basename) = os.path.split(coverage)

    # Paths and file object for the two output files (length and one polyA)

    # Open the respective files if set in the settings
    length_outpath = os.path.join(output_dir, 'length_'+dset_id)
    length_outfile = open(length_outpath, 'wb')

    # Only make a poly(A) file if you're going to write to it
    if polyA:
        polyA_outpath = os.path.join(output_dir, 'polyA_'+dset_id)
        polyA_outfile = open(polyA_outpath, 'wb')

    # list of PAS hexamers
    PAS_sites = ['AATAAA', 'ATTAAA', 'TATAAA', 'AGTAAA', 'AAGAAA', 'AATATA',
                 'AATACA', 'CATAAA', 'GATAAA', 'AATGAA', 'TTTAAA', 'ACTAAA',
                 'AATAGA']

    # compile regular expressions of PAS patterns
    pas_patterns = [re.compile(pas) for pas in PAS_sites]

    # Multi-exon dictionary to store unfinshed multi-exon utrs 
    multi_exons = {}
    # When all exons of a multi exon utr have been added, they will be
    # joined, performed calcluations on, and written to file like the 1exons.

    # First get line 1 as something to start from
    read_from = open(coverage, 'rb')

    line1 = read_from.next()
    (chrm, beg, end, utr_ID, val, strand, rel_pos, covr) = line1.split()

    # Create a UTR-instance
    this_utr = UTR(chrm, beg, end, strand, val, utr_ID, rpkm[utr_ID], covr,
                   polyA_reads[utr_ID], a_polyA_sites_dict[utr_ID],
                   utr_seqs[utr_ID])

    # Create instances for writing to two output files
    length_output = FullLength(utr_ID)
    if polyA:
        pAread_output = PolyAReads(utr_ID)

    # Write the headers of the length and polyA output files
    length_output.write_header(length_outfile)
    if polyA:
        pAread_output.write_header(polyA_outfile)

    # If tuning the cumulative length, open a file for this
    if settings.cumul_tuning:
        outfile = 'cumul_' + dset_id + '.stat'
        tuning_handle = open(os.path.join(output_dir, outfile), 'wb')
        #write header for the polyA-cumulative stats
        tuning_handle.write('\t'.join(['utr_id', 'epsilon_relative',
                                       'pA_to_cumul_dist', 'pA_cumul',
                                       'covr_minus', 'covr_plus', 'rpkm',
                                       'utr-length', 'strand']) + '\n')

    # Assert that the file information is the same as you started with
    assert feature_coords[utr_ID] == (chrm, int(beg), int(end), strand), 'Mismatch'

    # staring reading from line nr 2
    for line in read_from:
        (chrm, beg, end, utr_ID, val, strand, pos, covr) = line.split('\t')

        # Check if you stay on the same UTR-exon in the coverage file
        if utr_ID == this_utr.utr_ID:

            this_utr.covr_vector.append(int(covr))
            this_utr.cumul_covr.append(this_utr.cumul_covr[-1] + int(covr))

        # if not, this is the first line of a new UTR-exon
        else:
            calculate_and_write = True

            # check if the previous utr-exon was part of a multi-exon utr
            if this_utr.tot_exnr > 1:
                # If so, add to multi_exon archive
                utr_name = this_utr.utr_ID[:-2]

                if utr_name in multi_exons:
                    multi_exons[utr_name].append(this_utr)
                else:
                    multi_exons[utr_name] = [this_utr]

                # check if multi-exon is complete
                if len(multi_exons[utr_name]) == this_utr.tot_exnr:
                    # If complete, join the utrs into a this_utr object
                    this_utr = join_multiexon_utr(multi_exons[utr_name],
                                                  total_nr_reads)

                    # Create instances for writing to file
                    length_output = FullLength(utr_ID)
                    if polyA:
                        pAread_output = PolyAReads(utr_ID)

                else:
                    # if incomplete, continue without writing anything
                    calculate_and_write = False

            if calculate_and_write:

                # Calculate output values: 99.5% length, cumulative coverage, etc
                length_output.calculate_output(pas_patterns, this_utr)
                length_output.write_output(length_outfile, this_utr)

                if polyA:
                    # If the utr is empty, skip the poly(A) writing
                    if this_utr.is_empty():
                        pass

                    for side in ['plus_strand', 'minus_strand']:
                        # skip if empty
                        if this_utr.polyA_reads[side][0] == []:
                            continue

                        # calculate output for this strand
                        pAread_output.calculateA_output(pas_patterns, this_utr,
                                                        side)

                        # Save output to files
                        pAread_output.writeA_output(polyA_outfile, this_utr, side)

                # If tuning, calculate the tuning variables and write to file
                # DEPRECATED if you'll ever need it, though, it's here
                #if settings.cumul_tuning:
                    #calc_write_tuning(tuning_handle, length_output, this_utr)

            # Update to the new utr and start the loop from scratch
            this_utr = UTR(chrm, beg, end, strand, val, utr_ID, rpkm[utr_ID],
                           covr, polyA_reads[utr_ID],
                           a_polyA_sites_dict[utr_ID], utr_seqs[utr_ID])

            # If not a multi exon, make output instances for writing to file
            if not this_utr.multi_exon:
                length_output = FullLength(utr_ID)
                if polyA:
                    pAread_output = PolyAReads(utr_ID)

            # Assert that the next utr has correct info
            assert feature_coords[utr_ID] == (chrm, int(beg), int(end),
                                         strand), 'Mismatch'
    # Close opened files
    length_outfile.close()

    if settings.cumul_tuning:
        tuning_handle.close()

    if polyA:
        polyA_outfile.close()

def calc_write_tuning(tuning_handle, length_output, this_utr):
    """
    Write parameters for tuning them. Used to determine the epsilon value for
    finding the cut-off where an UTR ends, and also for the before/after
    coverage ration of the polyA clusters.
    """

    if this_utr.is_empty():
        return

    # Get the absolute end-position of the 99.5%, relative to the extended UTR
    end_pos = this_utr.epsilon_coord

    # Get the cumulative coverage and +/- 50nt coverage of the closest pA site
    write_output = False
    close_sites = []

    for pAsite in this_utr.polyA_reads[0]:
        if pAsite-50 < end_pos < pAsite+50:
            close_sites.append(pAsite)
            write_output = True

    if write_output:

         #There could be 3 clusters within 50 nt of the end_pos. You need to
         #choose the most downstream cluster.
        close_sites.sort()
        if this_utr.strand == '+':
            pAsite = close_sites[-1]
        if this_utr.strand == '-':
            pAsite = close_sites[0]

        # Get the absolute distance from the polyA site
        pA_dist = pAsite-end_pos

        # XXX now skipping the coverage on both sides of the polyA read. That
        # isn't very informative.

        ## Get the relative-to-extended position of the polyA site 
        ## HEY! you can't switch between genomic coordinates and covr_vector
        ## coordinates. For multi-3UTRs it might not matter at all.

        #rel_pA_pos = pAsite - this_utr.beg_ext

        #covr_vec = this_utr.covr_vector

        ## Get the coverage 50 nt on both sides of the polyA site
        ## No worried about extension -- it is in the end.
        #if this_utr.strand == '+':
            #d_stream_covr = sum(covr_vec[rel_pA_pos-50:rel_pA_pos])/float(50)
            #u_stream_covr = sum(covr_vec[rel_pA_pos:rel_pA_pos+50])/float(50)

        ## if negative strand and extended, become relative to the extended one
        #if this_utr.strand == '-':
            #if this_utr.extendby:
                #rel_ex_pA_pos = rel_pA_pos + this_utr.extendby
            #d_stream_covr = sum(covr_vec[rel_ex_pA_pos:rel_ex_pA_pos+50])/float(50)
            #u_stream_covr = sum(covr_vec[rel_ex_pA_pos-50:rel_ex_pA_pos])/float(50)

        # Get the cumulative coverage of the polyA site
        # However, watch out for reads that land outside the non-extended
        # region
        # For now I simply assign them to the last value of the region.. 
        # it's not satisfactory!

        #if (rel_pA_pos < 0) or (rel_pA_pos >= this_utr.length_nonext):
            #if this_utr.strand == '+':
                #cumul_pA = this_utr.norm_cuml[-1]
            #if this_utr.strand == '-':
                #cumul_pA = this_utr.norm_cuml[0]
        #else:
            #cumul_pA = this_utr.norm_cuml[rel_pA_pos]

        #d_stream_covr = str(d_stream_covr)
        #u_stream_covr = str(u_stream_covr)
        #cumul_pA = str(cumul_pA)

        pA_dist = str(pA_dist)
        rpkm = str(this_utr.rpkm)
        length = str(this_utr.length_nonext)
        default_pos = str(this_utr.rel_pos)
        strand = str(this_utr.strand)

        # Write To file
        tuning_handle.write('\t'.join([this_utr.utr_ID[:-2], default_pos,
                                       pA_dist, rpkm, length, strand]) + '\n')

        #tuning_handle.write('\t'.join([this_utr.utr_ID[:-2], default_pos,
                                       #pA_dist, cumul_pA, d_stream_covr,
                                       #u_stream_covr, rpkm, length, strand]) +
                            #'\n')

def verify_access(f):
    """
    Try to access file 'f'. If not successful, which means that the file either
    doesn't exist or you don't have access to it, abort program.
    """
    try:
        open(f, 'rb')
    except:
        print('Could not access {0}.\nCheck if file exists, and if it exists '\
              'check permissions'.format(f))
        sys.exit()

def read_settings(settings_file):
    """
    Read the settings and get all the settings parameters. These should be used
    to create a 'Settings' object.
    """

    conf = ConfigParser.SafeConfigParser()
    conf.optionxform = str
    conf.read(settings_file)

    expected_fields = ['DATASETS', 'ANNOTATION', 'CPU_CORES', 'RESTRICT_READS',
                       'CHROMOSOME1', 'SUPPLIED_3UTR_BEDFILE', 'HG_FASTA',
                       'POLYA_READS', 'MIN_3UTR_LENGTH', 'EXTEND', 'PLOTTING',
                       'UTR_LENGTH_TUNING', 'BIGWIG', 'GET_LENGTH']

    missing = set(conf.sections()) - set(expected_fields)

    if len(missing) == 0:
        pass
    else:
        print('The following options sections are missing: {}'.format(missing))
        sys.exit()

    # annotation
    try:
        annotation_path = conf.getboolean('ANNOTATION', 'annotation_path')
    except ValueError:
        annotation_path = conf.get('ANNOTATION', 'annotation_path')
        verify_access(annotation_path)
        annotation_format = conf.get('ANNOTATION', 'annotation_format')

    # annotated polyA sites
    try:
        annotated_polyA_sites = conf.get('ANNOTATION', 'annotated_polyA_files')
    except ValueError:
        annotated_polyA_sites = False
        verify_access(annotated_polyA_sites)

    # length yes or no
    length = conf.getboolean('GET_LENGTH', 'length')

    datasets = dict((dset, files.split(':')) for dset,
                    files in conf.items('DATASETS'))

    # Go through all the items in 'datsets'. Pop the directories from the list.
    # They are likely to be shortcuts.
    for (dset, dpaths) in datasets.items():
        for pa in dpaths:
            if os.path.isdir(pa):
                datasets.pop(dset)

    # check if the datset files are actually there...
    for dset, files in datasets.items():
        [verify_access(f) for f in files]

    # set minimum length of 3 utr
    try:
        min_utrlen = conf.getint('MIN_3UTR_LENGTH', 'min3utrlen')
    except ValueError:
        min_utrlen = 100

    # cpu cores
    try:
        max_cores = conf.getint('CPU_CORES', 'max_cores')
    except ValueError:
        max_cores = cpu_count()-1
        if max_cores < 1:
            max_cores = 1

    # restrict number of reads from source
    try:
        read_limit = conf.getint('RESTRICT_READS', 'restrict_reads')
    except ValueError:
        read_limit = conf.getboolean('RESTRICT_READS', 'restrict_reads')

    # supplied 3utr bedfile
    try:
        utr_bedfile_paths = conf.getboolean('SUPPLIED_3UTR_BEDFILE',
                                           'utr_bed_path')
    except ValueError:
        utr_bedfile_paths = conf.get('SUPPLIED_3UTR_BEDFILE',
                                    'utr_bed_path').split(':')
        [verify_access(path) for path in utr_bedfile_paths] # Check if is a file

    # If both 'annotation' and '3utr_bedfile' are false, your program isn't
    # going to go too far
    if not annotation_path and not utr_bedfile_paths[0]:
        print('You must either use a GENCODE annotation or supply a bedfile'\
              ' yourself. Not both can be false.')
        sys.exit()

    # restrict to chromosome 1
    chr1 = conf.getboolean('CHROMOSOME1', 'only_chr1')

    # human genome fasta file
    hg_fasta = conf.get('HG_FASTA', 'hg_fasta')

    # by how much should the 3utrs be extended?
    extendby = conf.get('EXTEND', 'extend_by')

    # bigWig or not?
    bigwig = conf.getboolean('BIGWIG', 'bigwig')

    bigwig_datasets = []
    bigwig_savedir = ''
    bigwig_url = ''
    if bigwig:
        bigwig_datasets = conf.get('BIGWIG', 'datasets').split(':')
        if bigwig_datasets[0] == 'all':
            bigwig_datasets = datasets.keys()
        bigwig_savedir = conf.get('BIGWIG', 'save_dir')
        bigwig_url = conf.get('BIGWIG', 'url')

    # poly(A) reads -- get them / dont get them / supply them
    try:
        polyA = conf.getboolean('POLYA_READS', 'polya')
    except ValueError:
        polyA = conf.get('POLYA_READS', 'polya')
        verify_access(polyA) # Check if is a file

    if not polyA and not length:
        print('You must have either "length" or "polya" (or both) as true ...)')
        sys.exit()

    # Get the gem_index_file
    gem_index = conf.get('POLYA_READS', 'gem_mapper_index')
    # If poly(A) reads == True, you need the index file for the gem-mapper
    if polyA == True:
        verify_access(gem_index+'.blf') # Check if is a file

    # tuning of the epsilon value. Write output file or not?
    cuml_tuning = conf.getboolean('UTR_LENGTH_TUNING', 'tuning')

    # if polyA is false, you can't do tuning
    if cuml_tuning and not polyA:
        print('To tune the cumulative value, you need to enable polyA reads')
        sys.exit()

    # if both an annotation and bedfiles are supplied, give the user warning
    # about this
    if annotation_path and utr_bedfile_paths:
        print('*'*80+'\nYou have supplied a path to an annotation as well as a '\
              'bedfile (or more) with genomic regions as input. The default '\
             'setting is to ignore the annotation in this case. If you want to '\
              'use the annotation and not the supplied bedfiles, set:\n\n'\
             '"utr_bed_path = false"\n\nin the settings file.\n'+'*'*80+'\n')

    return(datasets, annotation_path, annotation_format, utr_bedfile_paths,
           read_limit, max_cores, chr1, hg_fasta, polyA, min_utrlen, extendby,
           cuml_tuning, bigwig, bigwig_datasets, bigwig_savedir, bigwig_url,
           gem_index, length, annotated_polyA_sites)

def get_a_polyA_sites_path(settings, beddir):
    """
    Call the *annotation_parser* module to obtain the locations of all annotated
    utr-ends, in the program called 'a_polyA_sites'.

    Note that the user needs to specify the annotated polyA sites separately
    from the region-file. We cannot simply assume that the region files are
    3UTRs
    """

    # If options are not set, make them a 0-string define options
    if settings.chr1:
        chrm = '_chr1_'
    else:
        chrm = ''

    # If an annotation is provided, get polyA sites from the annotation
    if settings.annotation_path:
        aname = os.path.splitext(os.path.split(settings.annotation_path)[1])[0]

        a_polyA_sites_filename = 'a_polyA_sites' + chrm + aname + '.bed'
        polyA_site_bed_path = os.path.join(beddir, a_polyA_sites_filename)

        ## If the file already exists, don't make it again
        if os.path.isfile(polyA_site_bed_path):
            return polyA_site_bed_path

        if settings.chr1:
            settings.annotation_path = get_chr1_annotation(settings, beddir)

        t1 = time.time()
        print('Annotated polyA sites-file not found. Generating from annotation ...')
        genome.get_a_polyA_sites_bed(settings, polyA_site_bed_path)

        print('\tTime taken to generate polyA-bedfile: {0}\n'\
              .format(time.time()-t1))

    else:
        """ Since you're not getting information from the annotation, just make
        an empty file
        """
        handle = open(polyA_site_bed_path, 'wb')
        handle.close()

    return polyA_site_bed_path

def get_utr_path(settings, beddir, rerun_annotation_parser, region_file):
    """
    Get 3utr.bed-file path from annotation via annotation_parser module.
    """
    # Put together the name of the output file. The name depends on the options
    # that were used to get it.
    # If options are not set, make them a 0-string

    if settings.extendby:
        ext = '_extendby_max_'+str(settings.extendby)
    else:
        ext = ''

    if settings.chr1:
        chrm = '_chr1'
    else:
        chrm = ''

    if settings.regionfile_provided:
        user_provided = '_user_provided'
    else:
        user_provided = ''

    # utr path
    if not settings.regionfile_provided:
        utr_filename = '3UTR_exons' + chrm + ext + user_provided + '.bed'
        utr_bed_path = os.path.join(beddir, utr_filename)
    else:
        utr_filename = os.path.split(region_file)[1]
        utr_bed_path = os.path.join(beddir, utr_filename)

    # if you're not rerunning the annotation, check if file's already there
    if not rerun_annotation_parser:
        if os.path.isfile(utr_bed_path):
            return utr_bed_path

    ## Reload genome for any changes if asked for
    if rerun_annotation_parser:
        reload(genome)

    # If a regionfile is provided, never re-make from annotation, but shape it to
    if settings.regionfile_provided:
        utr_bed_path = shape_provided_bed(utr_bed_path, settings, region_file)

        return utr_bed_path

    # If regionfile is not provided, get it yourself from a provided annotation
    if not settings.regionfile_provided:
        if settings.chr1:
            settings.annotation_path_chr1 = get_chr1_annotation(settings, beddir)

        t1 = time.time()
        print('3UTR-bedfile not found. Generating from annotation ...')
        genome.get_3utr_bed_all_exons(settings, utr_bed_path)

        print('\tTime taken to generate 3UTR-bedfile: {0}\n'\
              .format(time.time()-t1))

    return utr_bed_path

def get_chr1_annotation(settings, beddir):
    """
    From the annotation, extract only entries for chromosome 1 and save to a
    separate file. This file is needed for speed-runs of the pipeline, while
    still giving relavant results.
    """

    (name, suffix) = os.path.splitext(os.path.basename(settings.annotation_path))
    filename = name + '_chr1' + suffix
    outpath = os.path.join(beddir, filename)

     #If the file already exists, don't make it again
    if os.path.isfile(outpath):
        return outpath

    t1 = time.time()
    print('Separating chr1 from the annotation ...')
    outhandle = open(outpath, 'wb')

    if settings.annotation_format == 'GENCODE':
        for line in open(settings.annotation_path, 'rd'):
            if line[:5] == 'chr1\t':
                outhandle.write(line)

    if settings.annotation_format == 'ENSEMBL':
        for line in open(settings.annotation_path, 'rd'):
            if line[:2] == '1\t':
                outhandle.write(line)

    outhandle.close()

    print('\tTime taken to separate chr1: {0}\n'.format(time.time()-t1))

    return outpath

def shape_provided_bed(utr_bed_path, settings, region_file):
    """
    Go through provided bedfile and shape it to confirm with internal standards
    in the program. Save in the bed-file directory.
    """

    outfile = open(utr_bed_path, 'wb')
    region_path = os.path.join(settings.region_dir, region_file)

    for line in open(region_path, 'rb'):
        (chrm, beg, end, name, val, strand) = line.split()[:6]

        end = int(end)
        beg = int(beg)

        # Skip lines that are not chrm1 if option is set
        if settings.chr1:
            if chrm not in ['chr1', 'Chr1']:
                continue

        # Skip short utrs
        # XXX not doing this. the person who provides should do it.
        #if end - beg < settings.min_utrlen:
            #continue

        outfile.write('\t'.join([chrm, str(beg), str(end), name, val,
                                 strand])+'\n')
    outfile.close()

    return utr_bed_path

def save_output(final_dict, output_dir):
    """
    Copy files in out_dict from the temp_files to the output folder. These are
    the key output files that you don't want to delete by accident.
    """

    for ID, final_path in final_dict.items():
        out_path = os.path.join(output_dir, os.path.basename(final_path))
        shutil.copyfile(final_path, out_path)

def get_rpkm(reads, utrfile_path, total_reads, utrs, extendby, dset_id):
    """
    Run coverageBed of reads on provided 3UTR and get RPKM. The RPKM is for the
    un-extended 3UTR. You need: total number of reads, length of the UTR, and
    the number of reads landing in this 3UTR. The rpkm measure has its problems
    when it comes to comparison between samples, but it's easy to calculate, and
    it tells you about expression within one experiment.
    """
    # Information needed for rpkm:
    # TOTAL reads
    # Length of UTR
    # # of reads landing in the UTR.
    rpkm = {}

    # If the bed-file has been extended, we need to unextend it. HOWEVER, we
    # should ONLY un-extend the 3' exons of the UTR. Internal exons have not
    # been extended and should not be touched.
    if extendby:
        temp_bed_path = os.path.join(os.path.dirname(utrfile_path), dset_id+
                                     '_temp_bed')
        temp_handle = open(temp_bed_path, 'wb')
        for line in open(utrfile_path, 'rb'):

            (chrm, beg, end, utr_id, val, strand) = line.split()

            # Reverse the extensions so you get correct RPKM!
            # Those that are extended have a val= 1+1000 to show that they are
            # extended. Those that are nto extened have a 1+0

            extended_by = int(val.split('+')[1])

            if strand == '+':
                end = int(end) - extended_by
            if strand == '-':
                beg = int(beg) + extended_by

            temp_handle.write('\t'.join([chrm, str(beg), str(end), utr_id, val,
                                         strand]) + '\n')
        temp_handle.close()

        utrfile_path = temp_bed_path

    p = Popen(['coverageBed', '-a', reads, '-b', utrfile_path], stdout=PIPE)

    for line in p.stdout:
        (chrm, bl, bl, utr_id, d, d, reads_covering) = line.split('\t')[:7]
        utr_length = utrs[utr_id][2] - utrs[utr_id][1]
        rpkm[utr_id] = ((10**9)*int(reads_covering))/(total_reads*utr_length)

    # Delete the tempfile
    if extendby:
        os.remove(temp_bed_path)

    return rpkm

def process_reads(pA_reads_path):
    """
    Remove reads that are too short or have a poor nucleotide composition.
    """
    processed_reads = os.path.splitext(pA_reads_path)[0]+'_processed.fas'
    outfile = open(processed_reads, 'wb')

    # Go through the file two lines at a time. If the next line does not begin
    # with '>', append line to last entry (it's a split fasta-file).

    length_sum = 0
    tot_reads = 0

    for line in open(pA_reads_path, 'rb'):
        # add lines until there are 2 entries in linepair
        (seq, tail, at) = line.split('#')
        at = at.strip('\n') # remove linebreak

        seqlen = len(seq)
        if seqlen > 25:
            As = seq.count('A')
            Ts = seq.count('T')
            # only save if A/T frequencies are not abnormal
            if (As/seqlen < 0.70) and (Ts/seqlen < 0.70):
                length_sum += seqlen
                tot_reads += 1
                # write to fasta format
                outfile.write('>{0}\t{1}\n'.format(at, tail)+seq+'\n')

    if tot_reads > 0:
        avrg_len = length_sum/float(tot_reads)
    else:
        avrg_len = 0

    outfile.close()

    return processed_reads, avrg_len

def map_reads(processed_reads, avrg_read_len, settings):
    """
    Map the processed reads using gem-mapper. Use the average read length to
    determine the number of mismatches for the mapper according to the following
    scheme where X is the average read length:
        * if X < 50, then 1 mismatch
        * if 50 < X < 100, then 2 mismatchs
        * if 100 < X, then 3 mismatches
    Regardless, only accept uniquely mapping reads.
    """

    # File path of the mapped reads
    mapped_reads = os.path.splitext(processed_reads)[0]+'_mapped'

    # Naming the final output
    polybed_path = os.path.splitext(processed_reads)[0] + '_mapped.bed'

    # How many mismatches depend on read length
    if avrg_read_len < 50:
        mismatch_nr = 1
    elif 50 < avrg_read_len < 100:
        mismatch_nr = 2
    elif 100 < avrg_read_len:
        mismatch_nr = 3

    ### mapping trimmed reads
    command = "gem-mapper -I {0} -i {1} -o {2} -q ignore -m {3}"\
            .format(settings.gem_index, processed_reads, mapped_reads, mismatch_nr)

    # XX REMOVE ME!!! JUST TO SKIP THE MAPPING DEBUG!! XXX 
    if not os.path.isfile(mapped_reads + '.0.map'):
        p = Popen(command.split())
        p.wait()
    else:
        pass

    # Accept mismatches according to average read length
    acceptables = {1: set(('1:0', '0:1')), 2: set(('1:0:0', '0:1:0', '0:0:1')),
                   3: set(('1:0:0:0', '0:1:0:0', '0:0:1:0', '0:0:0:1'))}

    acceptable = acceptables[mismatch_nr]
    getstrand = {'R':'-', 'F':'+'}
    start_re = re.compile('[0-9]*')

    reads_file = open(polybed_path, 'wb')

    # count the number of noisy reads and total reads
    noisecount = 0
    allcount = 0

    for line in open(mapped_reads + '.0.map', 'rb'):
        (at, tail, seq, mapinfo, position) = line.split('\t')

        # Acceptable reads and poly(A) reads are mutually exclusive.
        if mapinfo in acceptable:
            allcount += 1
            # Get chromosome, strand, and beg
            (chrom, rest) = position.split(':')
            strand = getstrand[rest[0]]
            beg = start_re.match(rest[1:]).group()

            # Determine the cleavage site and strand of the read

            # it came from the - strand
            if (at == 'T' and strand == '+') or (at == 'A' and strand == '-'):
                realStrand = '-'
                cleaveSite = beg

            # it came from the + strand
            if (at == 'T' and strand == '-') or (at == 'A' and strand == '+'):
                realStrand = '+'
                cleaveSite = str(int(beg)+len(seq))

            ## Don't write if this was a noisy read
            if read_is_noise(chrom, realStrand, int(cleaveSite), seq, at, tail,
                             settings):
                noisecount += 1
                continue

            # Store the seq-file with the PAS-cluster. You will align them in
            # downstream code.

            # Write to file in .bed format
            name = '#'.join([at, tail, seq])
            end = str(int(cleaveSite) + 1)

            reads_file.write('\t'.join([chrom, cleaveSite, end, name,
                                      '0', realStrand]) + '\n')

    # close file
    reads_file.close()

    # Write to logfile
    if allcount > 0:
        vals = (noisecount, allcount, noisecount/float(allcount))

        noiseinf = '\nNoise reads: {0}\nTotal reads: {1}\nNoise ratio: {2:.2f}\n'\
               .format(*vals)

        noiselog = open('NOISELOG.LOG', 'ab')
        noiselog.write('-'*80+'\n')
        noiselog.write(polybed_path)
        noiselog.write(noiseinf)
        noiselog.write('-'*80+'\n')
        noiselog.close()

    return polybed_path

def get_early_PAS(seq, at, pas_patterns):
    """
    Get the PAS and the distance of the putative poly(A) read.
    """

    # if it came from a T-read, reverse-complement first
    if at == 'T':
        seq = reverseComplement(seq)

    PAS, PAS_dist = get_pas_and_distance(pas_patterns, seq)
    # you are looking for the distance from the end

    if PAS != 'NA':
        PAS_dist = [len(seq) - dist for dist in PAS_dist]

    return PAS, tuple(PAS_dist)

def read_is_noise(chrm, realStrand, cleaveSite, seq, at, tail, settings):
    """
    Get the original sequence. If the A/T count on the geneomic sequence is
    within 30% of the A/T count on the genome, ignore the read. This corresponds
    to 4:1, 5:1, 6:1, 7:2, 8:2, 9:2, 10:3, 11:3, 12:3, 14:, 4
    40 % would be
    to 4:1, 5:2, 6:2, 7:2, 8:4, 9:3, 10:4, 11:4, 12:5, 14:, 5
    of course, this is a simply solution. In fact, the longer the tail, the less
    likely it is that the reads follow each other. But this will have to do.

    There are two scenarios of genomic noise:

        1)
        GGCCCAAAAAAAA
        CCGGGTTTTTTTT

    and

        2)
        TTTTTTTTGGAC
        AAAAAAAACCTG

    (XXXXTTTT on + and TTTTXXXX on - don't apply because you look for A-ends and
    T-heads)

    In 1), you classify this as a poly(A) read on the + strand. Thus, you need
    to check the + sequence where the tail is for As.
    in 2), you classify this as a poly(A) read on the - strand. Thus, check the
    - strand for As.

    However, keep in mind that you accept all kinds of reads, irrespective of
    quality measure. God, to do this properly, you need to take qualities into
    account ...
    """
    frac = 0.3 # if the polyA tail has 10A, accept 6 or fewer As in the genomic
    # sequence.

    # get the sequences! note: the sequence fetcher automatically
    # reverse-transribes regions on the - strand. In other words, it
    # always returns sequences in the 5->3 direction.

    tailCount = tail.count(at)

    tail_len = len(tail)

    ## it came from the - strand
    if realStrand == '-':
        # get beg-strip_len:beg
        seq_dict = {'seq': (chrm, cleaveSite - tail_len - 1, cleaveSite,
                            realStrand)}
        genomeSeq = genome.get_seqs(seq_dict, settings.hgfasta_path)['seq']

    ## it came from the + strand
    if realStrand == '+':
        # get beg+seq_len:beg+seq_len+strip_len
        seq_dict = {'seq': (chrm, cleaveSite, cleaveSite + tail_len + 1,
                            realStrand)}
        genomeSeq = genome.get_seqs(seq_dict, settings.hgfasta_path)['seq']

    genomeAs = genomeSeq.count('A')

    winsize = int(math.floor(genomeAs*frac))
    window = range(genomeAs - winsize, genomeAs+1)

    print (tail, 'tail')
    print (genomeSeq, 'genomic')
    print genomeAs
    print window

    # return as true noise 
    if tailCount in window:
        print 'Noise'
        debug()
        return True
    # return as not-noise
    else:
        print 'Not noise!'
        debug()
        return False


def get_bed_reads(dset_reads, dset_id, read_limit, tempdir, polyA, get_length,
                  polyA_path):
    """
    Get reads from file. Determine file-type. If gem, extract from gem and
    convert to bed. If .bed, concatenate the bedfiles and convert them to the
    desired internal format.
    """

    # Path of .bed output
    out_path = os.path.join(tempdir, 'reads_'+dset_id+'.bed')

    # allowed suffixes:
    # [gem, map, gz]
    # [gem, map]
    # [bed]
    # [bed.gz]
    ok_sufx = ['gem', 'map', 'gz', 'bed']

    # Building a suffix: Start with a '.' separated list of
    # the file name. Proceed backwards, adding to reverese_suffix if the entry
    # is in the allowed group.
    dotsplit = os.path.basename(dset_reads[0]).split('.')
    suflist = [sufpart for sufpart in reversed(dotsplit) if sufpart in ok_sufx]

    suffix = '.'.join(reversed(suflist))

    # If in gem-format, go through the file with zcat -f
    if suffix in ['gem.map.gz', 'gem.map']:
        print('Obtaining reads from mapping for {0} ...\n'.format(dset_id))
        # Get only the uniquely mapped reads (up to 2 mismatches)
        bundle = zcat_wrapper(dset_id, dset_reads, read_limit, out_path, polyA,
                                   polyA_path, get_length)

        (total_mapped_reads, total_reads, acount, tcount) = bundle

    # If in bed-format, also run zcat -f on all the files to make one big
    # bedfile. How to restrict reads in this case? No restriction?
    elif suffix in ['bed.gz', 'bed']:
        total_reads = concat_bedfiles(dset_reads, out_path, polyA, polyA_path)
        acount = 0
        tcount = 0

    else:
        print('Non-valid suffix: {0}. Allowed suffixes are .gem.map.gz,\
              .gem.map, .gem.map.gz, .bed.gz, and .bed'.format(suffix))
        sys.exit()

    return (out_path, polyA_path, total_reads, total_mapped_reads, acount, tcount)

def concat_bedfiles(dset_reads, out_path, polyA, polyA_path):
    """
    Throw zcat on the bedfiles. With the -f option, it will concatenate both
    compressed and noncompressed files.
    """

    # File objects for writing to
    out_file = open(out_path, 'wb')
    polyA_file = open(polyA_path, 'wb')


    cmd = ['zcat', '-f'] + dset_reads
    f = Popen(cmd, stdout=out_file)
    f.wait()

    # Determine the bed format (3, 4, 5 or 6). If less than 6, convert to 6.
    field_nr = len(open(out_path, 'rb').next().split())
    if field_nr < 6:
        print('\nIncoming bedfile has {0} fields. Converting to 6 fields ...\n'\
              .format(field_nr))
        bed_field_converter(out_path, field_nr)

    if field_nr > 6:
        print('\nConvert your bedfile to contain only the 6 first fields.')
        sys.exit()

    # If polyA is a path to a bedfile (or bedfiles) concatenate these too
    if type(polyA) == str or type(polyA) == unicode:
        cmd = ['zcat', '-f'] + polyA
        f = Popen(cmd, stdout=polyA_file)
        f.wait()

    # Return the number of line numbers (= number of reads)
    return sum(1 for line in open(out_path, 'rb'))

def bed_field_converter(out_path, field_nr):
    # Get how many fields to add
    addme = '\t'.join(['0', '0', '+'][5-field_nr:]) + '\n'
    # Formats are chrom, beg, end, name, score, and strand
    # if 3, add 0, 0, +
    # if 4, add 0, +
    # if 5, add +
    # if more than 6 type warning: only bedfiles with 6 or fewer fields accepted.

    # Write to a temp file
    temp_path = out_path+'TEMP'
    with open(temp_path, 'wb') as temp_file:
        for line in open(out_path, 'rb'):
            temp_file.write(line.rstrip('\n') + addme)

    # Move the temp file to the original file
    os.rename(temp_path, out_path)

def get_at_counts(polyAbed, utrfile_path):
    """
    Get counts of A and T reads that map and how many map to features in your
    genomic region
    """
    # A dictionary to hold the utr_id -> poly(A)-reads relation
    amapped = 0
    tmapped = 0

    amapped_tofeature = 0
    tmapped_tofeature = 0

    utr_polyAs = {}

    # Hey, we do know the strand ... but maybe we're not interested: no -s
    cmd = ['intersectBed', '-wa', '-wb', '-a', polyAbed, '-b', utrfile_path]

    # Run the above command -- outside the shell -- and loop through output
    f = Popen(cmd, stdout=PIPE)
    for line in f.stdout:

        polyA, utr = line.split()[:7], line.split()[7:]

        # save the intersection of poly(A) sites 
        utr_id = utr[3]

        # add all poly(A) sites to each region element
        if not utr_id in utr_polyAs:
            utr_polyAs[utr_id] = [tuple(polyA)]
        else:
            utr_polyAs[utr_id].append(tuple(polyA))

        if polyA[3] == 'T':
            tmapped += 1

            if polyA[-1] != utr[-1]:
                tmapped_tofeature += 1

        if polyA[3] == 'A':
            amapped += 1

            if polyA[-1] == utr[-1]:
                amapped_tofeature += 1

    at_bundle = (amapped, tmapped, amapped_tofeature, tmapped_tofeature)

    return at_bundle, utr_polyAs

def cluster_loop(ends):
    """
    Cluster the polyA reads together. It is assumed that the list 'ends' comes
    in as sorted. Go through the list; the first site is a cluster; if the
    second site is within 20nt, it also becomes a cluster, and the new cluster
    site is the average of the two sites. If the next site were not withing 20
    nt, then this previous site is kept as a cluster, and the new site is the
    new cluster; and so on.

    Keep the tail information. Average the nucleotide counts with the number of
    reads in the cluster. (and sub-clustering? and subsub-clustering? Just keep
    averaging I suppose ... ... codemonkey :)

    Damn: a proof against this could have been that reads landing at the same
    site have different tails. This would show a random process (or sequencing
    error), instead of spliced reads. Another argument against split reads would
    be the average length of the tails. If that length is short, it implies not
    split reads but something else.
    """

    # If you get an empty set in, return emtpy out
    if ends == []:
        return [[], [], []]

    clustsum = 0
    clustcount = 0
    this_cluster = []
    clusters = []
    for indx, (val, name) in enumerate(ends):
        ival = int(val)

        clustsum = clustsum + ival
        clustcount += 1
        mean = clustsum/clustcount

        # If dist between new entry and cluster mean is < 20, keep in cluster
        if abs(ival - mean) < 20:
            this_cluster.append((ival, name))

        else: # If not, start a new cluster, and save the old one
            clusters.append(this_cluster)
            clustsum = ival
            clustcount = 1
            this_cluster = [(ival, name)]

    # Append the last cluster
    clusters.append(this_cluster)

    # Get the mean of the clusters
    cluster_avrg = []

    # Store the average tail info from the clusters
    nucsums = []

    # store all sequences
    seqs = []

    for clus in clusters:
        # skip empty clusters
        if clus == []:
            continue

        (at, tail_info, seq) = name.split('#')

        debug()

        # The averaged location of the clusters
        clu_sum = sum([k[0] for k in clus])
        cluster_avrg.append(int(math.floor(clu_sum/len(clus))))

        nucsum = {'G':0, 'A':0, 'T':0, 'C':0}
        # Add all nucleotides to the sum
        nuc_dicts = [dict([b.split('=') for b in k[1].split(':')]) for k in clus]
        for ndict in nuc_dicts:
            for nucl, nr in ndict.items():
                nucsum[nucl] += int(nr)

        # Average the nucsum (first name int -> str)
        tmp = []
        for nuc, nr in nucsum.items():
            tmp.append('='.join([nuc, str(nr/len(clus))]))
        nucsum_avrg = ':'.join(tmp)

        nucsums.append(nucsum_avrg)

    # Strip the tail info from the clusters
    new_clusters = []
    for subcl in clusters:
        new_subcl = [c[0] for c in subcl]
        new_clusters.append(new_subcl)

    # Return the clusters and their average
    return (cluster_avrg, new_clusters, nucsums)

def cluster_polyAs(settings, utr_polyAs, utrfile_path, utrs, polyA):
    """
    For each UTR, save the clustered poly(A)-reads from both strands. For
    double-stranded reads, it is known that the poly(A) reads map to the
    opposite side of the strand it came from. You count the number of reads
    landing in different strands and use the ratio as a false-positive rate. You
    also count the number of identical reads in both strands as a measure of how
    many on the self-strand are genuine (if you know the orientation of the
    original genomic feature, that is)!

    Since in recent updates you output the nucleotide composition of the trimmed
    tails, you need to save these somehow. Update the algorithm by sorting
    tuples. The second piece of information in the tuple is the tail-info. This
    information must be stacked. Should you keep all the information or just the
    average number of nucleotides in the tails? Maybe average is enough.

    A more recent update includes the PAS found on the poly(A) read itself. This
    is for comparing with the PAS'es in the genome reference sequence.
    """

    # If there are no polyA reads or polyA is false: return empty lists 
    if polyA == False or utr_polyAs == {}:
        polyA_reads = dict((utr_id, {'plus_strand':[[],[], []],
                                     'minus_strand':[[], [], []]})
                           for utr_id in utrs)
        return polyA_reads

    polyA_reads = {}

    # expand the poly(A) sites 

    for utr_id, polyAs in utr_polyAs.iteritems():

        plus_sites = []
        minus_sites = []

        # you know the strand of the poly(A) reads from the A/T and strand they
        # map to 

        for tup in polyAs:
            (chrm, beg, end, name, val, strand) = tup
            (at, tail_info, PAS, PAS_dist) = name.split('#')

            # it came from the - strand
            if  strand == '-':
                minus_sites.append((beg, name))

            # it came from the + strand
            if  strand == '+':
                plus_sites.append((beg, name))

        polyA_reads[utr_id] = {'plus_strand': cluster_loop(sorted(plus_sites)),
                               'minus_strand': cluster_loop(sorted(minus_sites))}

    # For those utr_id that don't have a cluster, give them an empty list; ad hoc
    for utr_id in utrs:
        if utr_id not in polyA_reads:
            polyA_reads[utr_id] = {'plus_strand': [[],[], []],
                                   'minus_strand': [[],[], []]}

    return polyA_reads

def downstream_sequence(settings, polyA_clusters, feature_coords):
    """
    For each of the poly(A) clusters, get the nucleotide sequence 40 nt
    downsteam. Look for PAS, and determine their distance and nature, preferably
    in sub-functions that can be reused by the 3UTR length output branch.
    """
    # each utr or genomic element gets its sequence dictionary. the indexes of
    # the subdict are the polyA sites themselves
    # pA_seqs[region_id][plus_strand][pA_coordinate] = 'GATTACA' for example

    # for getting strand:
    strandgetter = {'plus_strand': '+',
                    'minus_strand': '-'}

    #pA_seqs = genome.get_seqs(annotation.feature_coords, settings.hgfasta_path)
    pA_seqs = {}
    for reg_id, pm_dict in polyA_clusters.iteritems():
        orient_dict = {}

        for orientation, (centers, clusters, tail_info) in pm_dict.items():

            # skip the ones without sequence
            if centers == []:
                continue
            # we need to send in a dict[pA_coord] = (chrm, beg, beg+1, strnd)
            # NOTE DICT IS HASHED BY THE BEG COORDINATE, IRRESPECTIVE OF STRAND
            # UTILIZE THIS IFARMATIONZ l8r
            orient_dict[orientation] = {}
            center_dict = {}
            for center in centers:
                chrm = feature_coords[reg_id][0]

                if strandgetter[orientation] == '+':
                    beg = center - 40
                    end = center
                    strand = '+'
                else:
                    beg = center
                    end = center + 40
                    strand = '-'

                center_dict[center] = (chrm, beg, end, strand)

            # get the sequences! note: the sequence fetcher automatically
            # reverse-transribes regions on the - strand. In other words, it
            # always returns sequences in the 5->3 direction.
            orient_dict[orientation] = genome.get_seqs(center_dict,
                                                       settings.hgfasta_path)
        pA_seqs[reg_id] = orient_dict

    return pA_seqs

def pipeline(dset_id, dset_reads, tempdir, output_dir, settings, annotation,
             DEBUGGING, polyA_cache, here):
    """
    Get reads, get polyA reads, cluster polyA reads, get coverage, combine it in
    a 3UTr object, do calculations on the object attributes, write calculation
    to output files ... this is where it all happens: the PIPEline.
    """

    # Give new names to some parameters to shorten the code
    utrfile_path = annotation.utrfile_path
    extendby = settings.extendby
    read_limit = settings.read_limit
    polyA = settings.polyA
    get_length = settings.get_length
    feature_coords = annotation.feature_coords

    # Define utr_polyAs in case polyA is false
    utr_polyAs = {}

    t0 = time.time()

    # Define the path of the polyA file
    just_dset = '_'.join(dset_id.split('_')[:-1])
    polyA_path = os.path.join(tempdir, 'polyA_reads_'+just_dset+'.fa')

    # if the poly(A) reads have already been mapped and saved to poly(A) cache,
    # don't get them again

    if polyA_cache:

        if DEBUGGING:
            cache_dir = os.path.join(os.path.join(here, 'DEBUGGING'),
                                     'polyA_cache')
        else:
            cache_dir = os.path.join(here, 'polyA_cache')

        if not os.path.isdir(cache_dir):
            os.makedirs(cache_dir)

        polydone_path = os.path.splitext(os.path.split(polyA_path)[1])[0]
        polyA_cached_path = os.path.join(cache_dir, polydone_path+\
                                         '_processed_mapped.bed')

        # If the file exists, make this the poly(A) path
        # XXX: I think there is a bug if the file exists but it is empty: it
        # holds no reads.
        if os.path.isfile(polyA_cached_path):
            polyA = polyA_cached_path

    # Get the normal reads (in bed format), if this option is set. Get the polyA
    # reads if this option is set. As well get the total number of reads for
    # calculating the RPKM later. p_polyA_bed stands for putative polyA reads
    # If length is false: don't get normal reads.

    all_reads = get_bed_reads(dset_reads, dset_id, read_limit, tempdir,
                              polyA, get_length, polyA_path)

    # Extract from all_reads. polyA might have been modified from True to
    # 'some_path' if cached polyA reads have been found
    (bed_reads, p_polyA_bed, total_reads, total_mapped_reads, acount, tcount) =\
            all_reads

    # update acount and tcount from saved file if polyAcache is on
    if polyA_cache:
        at_count_path = os.path.join(cache_dir,polydone_path+'_atcount')

        if os.path.isfile(at_count_path):
            (total_reads, acount, tcount) = open(at_count_path,'rb').readline()\
                    .split()

    # If polyA is a string, it is a path of a bedfile with the polyA sequences
    if type(polyA) == str or type(polyA) == unicode:
        polyA_bed_path = polyA
        # Get numbers of As and Ts for a log file
        at_numbers, utr_polyAs = get_at_counts(polyA_bed_path, utrfile_path)

    # If polyA is True, trim the extracted polyA reads, remap them, and save the
    # uniquely mapping ones to bed format
    elif polyA == True:
        # PolyA pipeline: remove low-quality reads, remap, and -> .bed-format:

        # 1) Process reads by removing those with low-quality, and removing the
        #    leading Ts and/OR trailing As.
        processed_reads, avrg_read_len = process_reads(p_polyA_bed)

        # 2) Map the surviving reads to the genome and return unique ones
        print('Remapping poly(A)-reads for {0} + noise filtering...'.format(dset_id))
        polyA_bed_path = map_reads(processed_reads, avrg_read_len, settings)

        # 3) If caching is on, save the polyA_bed_path to the cache dir
        if polyA_cache and read_limit == False:
            shutil.copyfile(polyA_bed_path, polyA_cached_path)

            # also save the acount and tcount
            at_count_path = os.path.join(cache_dir,polydone_path+'_atcount')

            outhandle = open(at_count_path, 'wb')
            outhandle.write('{0}\t{1}\t{2}'.format(total_reads, acount, tcount))
            outhandle.close()
        at_numbers, utr_polyAs = get_at_counts(polyA_bed_path, utrfile_path)

    # Cluster the poly(A) reads for each utr_id. If polyA is false or no polyA
    # reads were found, return a placeholder, since this variable is assumed to
    # exist in downstream code. Further, save the tail information in the
    # clustering somehow

    if polyA:
        print('Clustering poly(A) reads ...')
        polyA_clusters = cluster_polyAs(settings, utr_polyAs, utrfile_path,
                                        feature_coords, polyA)

    if polyA:
        tx = time.time()
        print('Fetching the sequences downstream poly(A) sites ...')
        pA_seqs = downstream_sequence(settings, polyA_clusters, feature_coords)
        print('\tTime to get sequences: {0}\n'.format(time.time() - tx))

    if polyA:
        output_path = os.path.join(output_dir, dset_id+'_polyA_statistics')
        write_polyA_stats(polyA_clusters, acount, tcount, at_numbers,
                          total_reads, feature_coords, output_path, settings)

    if get_length:
        # Get the sequences of the genomic regions in question; you'll be
        # looking for poly(A) sites in them
        utr_seqs = genome.get_seqs(annotation.feature_coords, settings.hgfasta_path)
        # Get the RPKM, if you get lengths
        print('Obtaining RPKM for {0} ...\n'.format(dset_id))
        rpkm = get_rpkm(bed_reads, utrfile_path, total_mapped_reads, feature_coords,
                        extendby, dset_id)

        # Cover the 3UTRs with the reads, if you get lengths
        print('Getting read-coverage for the 3UTRs for {0} ...\n'.format(dset_id))
        coverage_path = coverage_wrapper(dset_id, bed_reads, utrfile_path)

        # Collect read coverage for each 3UTR exon, join the multi-exon 3UTRs,
        # and write to file the parameters you calculate
        print('Writing output files... {0} ...\n'.format(dset_id))
        output_writer(dset_id, coverage_path, annotation, rpkm, polyA_clusters,
                      settings, total_mapped_reads, output_dir, polyA, utr_seqs)

    else:
        only_polyA_writer(dset_id, annotation, pA_seqs, polyA_clusters,
                          settings, output_dir)

    print('Total time for {0}: {1}\n'.format(dset_id, time.time() - t0))

    return 1


def write_polyA_stats(polyA_reads, acount, tcount, at_numbers, total_nr_reads,
                      feature_coords, output_path, settings):
    """
    File for outputting basic polyA statistics. These statistics make most sense
    in 3UTR regions, because we know the orientation of the transcripts. In
    other regions, they don't make sense at all, because you don't know the
    direction of the transcripts. However, if in some region you find that the
    ratio is enriched for one strand, it is likely that this region has true
    poly(A) sites.

    1) Estimated false-positive rate (this_strand/other_strand)

    2) Estimated false-negative rate (common to both strands) (but these are
    actually added to the poly(A)-read pool)

    3) PAS/annotation rate of the so called false-negatives? Is it the same as
    for the others?

    4) distribution of ratio of unique_reads/total_reads for each cluster site

    5) distribution of standard deviations about the cluster mean.

    6) distribution of the number of unique reads for each cluster

    7) number of 3UTRs with poly(A) reads detected. useful for comparing to the
    number which have read coverage ... an estimate of how many of the 3UTRs you
    manage to reach. Number should improve by using both biological replicates.

    8) Distribution of reads from the other strand -- do they arrive in
    clusters? How does this look compared to other_strand?

    IDEA: get the distribution of the unique reads about the center. This should
    reflect the cleavage process.
    """

    # If you don't have any reads, you can do nothing here 
    if polyA_reads == {}:
        handle = open(output_path, 'wb')
        handle.write('No poly(A) reads mapped')
        handle.close()
        return

    this_strand_count = 0
    other_strand_count = 0
    both_strand_count = 0
    both_by_chance = 0

    clusters = 0

    # Lists of unique vs. total reads for each utr for each strand
    distributions = {'this_strand': {'unique_reads': [], 'total_reads': [],
                                  'spread': []},
                     'other_strand': {'unique_reads': [], 'total_reads': [],
                                   'spread': []}}

    for utr, strand_stats in polyA_reads.items():
        # Skip those without poly(A) reads
        if strand_stats == {'plus_strand': [[], []], 'minus_strand': [[], []]}:
            continue

        clusters += len(strand_stats['minus_strand'][0])
        clusters += len(strand_stats['plus_strand'][0])

        if feature_coords[utr][3] == '+':
            this_strand_reads = sum(strand_stats['plus_strand'][1], [])
            other_strand_reads = sum(strand_stats['minus_strand'][1], [])
            other_strand_clstrs = strand_stats['minus_strand'][0]
        else:
            this_strand_reads = sum(strand_stats['minus_strand'][1], [])
            other_strand_reads = sum(strand_stats['plus_strand'][1], [])
            other_strand_clstrs = strand_stats['plus_strand'][0]

        this_strand_count += len(this_strand_reads)
        other_strand_count +=len(other_strand_reads)

        movers = 0

        # Getting those that are in both strands. Method: see if any of the
        # this_reads are close to cluster_centers
        if this_strand_reads != [] and other_strand_reads != []:
            for thi_end in this_strand_reads:
                for oth_cls in other_strand_clstrs:
                    if int(oth_cls)-15 < int(thi_end) < int(oth_cls)+15:
                        both_strand_count += 1
                        movers += 1

            # length of this exon
            exon_len = feature_coords[utr][2]-feature_coords[utr][1]

            # Number of clustersClusters in 
            cls_nr = len(other_strand_clstrs)

            # size of +/- 15 about those clusters
            target_size = cls_nr*30

            # probl of one of the movers hitting the target size
            # prob of landing in target size for 1 mover
            targ_land = target_size/exon_len

            # prob for all movers, assuming independence
            both_by_chance += targ_land*movers

    handle = open(output_path, 'wb')

    (amapped, tmapped, amapped_tofeature, tmapped_tofeature) = at_numbers

    # total nr of reads mapped
    handle.write('Total number of reads\t{0}\n'.format(total_nr_reads))

    #-1) A/T reads statistics
    handle.write('Nr of trailing A reads total\t{0}\n'.format(acount))
    handle.write('Nr of leading T reads total\t{0}\n'.format(tcount))
    handle.write('Nr of trailing A reads mapped\t{0}\n'.format(amapped))
    handle.write('Nr of leading T reads mapped\t{0}\n'.format(tmapped))
    handle.write('Nr of trailing A reads mapped to annotated object\t{0}\n'\
                 .format(amapped_tofeature))
    handle.write('Nr of leading T reads mapped to annotated object\t{0}\n'\
                 .format(tmapped_tofeature))

    # Write the total number of clusters
    handle.write('Nr of clusters\t{0}\n'.format(clusters))

    #0) The strand statistics (assuming the strand in the bedfile makes sense)
    handle.write('Annotated strand count\t{0}\n'.format(this_strand_count))
    handle.write('Other strand count\t{0}\n'.format(other_strand_count))
    handle.write('Both strands count\t{0}\n'.format(both_strand_count))
    handle.write('Both strands by chance\t{0}\n'.format(both_by_chance))

    #1) Estimated false-positive rate (this_strand/other_strand)
    if this_strand_count > 0:
        fals_pos = other_strand_count/this_strand_count
    else:
        fals_pos = 'NA'

    # I call it false positive because you can assume that the same rate of
    # reads will land in 
    handle.write('"False positive"-rate\t{0}\n'.format(fals_pos, '.4f'))

    #2) Estimated false-negative rate (common to both strands)
    if this_strand_count > 0:
        fals_neg = both_strand_count/this_strand_count
    else:
        fals_neg = 'NA'

    handle.write('"False negative"-rate\t{0}\n'.format(fals_neg, '.4f'))

    # count the number of clusters
    clusters = 0

    # Get distributions of the poly(A) reads you actually use
    for utr, strand_stats in polyA_reads.items():

        for keyw in ['this_strand', 'other_strand']:

            # Add total and unique reads for the clusters
            if feature_coords[utr][3] == '+' and keyw == 'this_strand':
                (cls_centers, cls_all_reads, info) = strand_stats['plus_strand']

            if feature_coords[utr][3] == '+' and keyw == 'other_strand':
                (cls_centers, cls_all_reads, info) = strand_stats['minus_strand']

            if feature_coords[utr][3] == '-' and keyw == 'this_strand':
                (cls_centers, cls_all_reads, info) = strand_stats['minus_strand']

            if feature_coords[utr][3] == '-' and keyw == 'other_strand':
                (cls_centers, cls_all_reads, info) = strand_stats['plus_strand']

            for cls_center, cls_reads in zip(cls_centers, cls_all_reads):
                distributions[keyw]['total_reads'].append(len(cls_reads))
                distributions[keyw]['unique_reads'].append(len(set(cls_reads)))
                std = math.sqrt(sum([(cls_center-cls_pos)**2 for cls_pos in
                           cls_reads])/len(cls_reads))
                distributions[keyw]['spread'].append(std)

    # Write the total number of clusters

    # Calculate key statistics from the this_strand and other_strand variables
    for strand_way, dist in distributions.items():

        # 2) average, std, and sum of total reads
        total_sum = sum(dist['total_reads'])
        if len(dist['total_reads']) > 0:
            total_mean = total_sum/len(dist['total_reads'])
            total_std = math.sqrt(sum([(tot-total_mean)**2 for tot in
                                       dist['total_reads']])/len\
                                  (dist['total_reads']))
        else:
            total_mean = 0
            total_std = 0

        handle.write('{0} Average total reads in clusters\t{1}\n'\
                     .format(strand_way, total_mean))
        handle.write('{0} Std total reads in clusters\t{1}\n'.format(strand_way,
                                                             total_std))
        # 3) average, std, sum of unique reads
        unique_sum = sum(dist['unique_reads'])
        if len(dist['unique_reads']) > 0:
            unique_mean = unique_sum/len(dist['unique_reads'])
            unique_std = math.sqrt(sum([(tot-unique_mean)**2 for tot in
                                       dist['unique_reads']])/len\
                                   (dist['unique_reads']))
        else:
            unique_mean = 0
            unique_std = 0

        handle.write('{0} Sum unique reads\t{1}\n'\
                     .format(strand_way, unique_sum))
        handle.write('{0} Average unique reads in clusters\t{1}\n'\
                     .format(strand_way, unique_mean))
        handle.write('{0} Std uniuqe reads in clusters\t{1}\n'\
                     .format(strand_way, unique_std))
    handle.close()

def only_polyA_writer(dset_id, annotation, pA_seqs, polyA_reads, settings,
                      output_dir):
    """ If only poly(A) is asked for, skip the footprint-heavy "outpt_writer"
    and use this special method for just writing poly(A) output.

    Thus you must remove the coverage and RPKM parameters from the
    length-output.

    NOTE: when running poly(A) checks on non-3UTR regions, you need to accept
    poly(A) reads from both strands. When running in 3UTR regions you should
    accept those from the other strand if they do overlap. Count the remainder
    as a crude measure of error rate?

    You probably shouldn't concatenate the exons from multi-exon 3UTRs.
    Therefore you will not get exactly the same output as in the other output
    writer. for example, polyA_number will go away.
    """

    output_order = ['chrm',
                    'beg',
                    'end',
                    'utr_ID',
                    'strand',
                    'polyA_coordinate',
                    'polyA_coordinate_strand',
                    'polyA_average_composition',
                    'annotated_polyA_distance',
                    'nearby_PAS',
                    'PAS_distance',
                    'PET_support',
                    'cufflinks_support',
                    'number_supporting_reads',
                    'number_unique_supporting_reads',
                    'unique_reads_spread']

    # list of PAS hexamers
    PAS_sites = ['AATAAA', 'ATTAAA', 'TATAAA', 'AGTAAA', 'AAGAAA', 'AATATA',
                 'AATACA', 'CATAAA', 'GATAAA', 'AATGAA', 'TTTAAA', 'ACTAAA',
                 'AATAGA']

    pas_patterns = [re.compile(pas) for pas in PAS_sites]

    # Get dict with annotated poly(A) sites and dict with utr exons
    pet_dict = annotation.PETdict
    cuff_dict = annotation.cufflinksDict

    a_polyA_sites_dict = annotation.a_polyA_sites_dict
    feature_coords = annotation.feature_coords

    polyA_outpath = os.path.join(output_dir, 'onlypolyA_'+dset_id)
    polyA_outfile = open(polyA_outpath, 'wb')

    # Write header to the columns
    polyA_outfile.write('\t'.join(output_order)+'\n')

    for feature_id, polyA_dict in polyA_reads.iteritems():
        # Skip those that don't have any reads in them 
        if polyA_dict == {'plus_strand': [[], []], 'minus_strand': [[], []]}:
            continue

        (chrm, beg, end, strand) = feature_coords[feature_id]
        annotated_pA_sites = a_polyA_sites_dict[feature_id]
        anyPet = pet_dict[feature_id]
        anyCuff = cuff_dict[feature_id]

        # you need to go through both plus and minus strands and write the
        # clusters for each. this will change a lot of downstream readings.
        for pm_strand, cluster_list in polyA_dict.items():

            # Get the strands of the poly(A) cluster
            if pm_strand == 'minus_strand':
                pA_coord_strand = '-'
            if pm_strand == 'plus_strand':
                pA_coord_strand = '+'

            for cls_center, cls_reads, tail_info in zip(*cluster_list):
                annotated_polyA_distance = annotation_dist(cls_center,
                                                           annotated_pA_sites,
                                                           strand)
                PET_support = PETorNot(cls_center, anyPet, strand)
                cufflinks_support = Cuffme(cls_center, anyCuff, strand)

                # getting PAS. If annotation exists, look at the annotated strand.
                # If not, look at the "other strand" than the poly(A) read maps to.
                sequence = pA_seqs[feature_id][pm_strand][cls_center]
                (nearby_PAS, PAS_distance) = get_pas_and_distance(pas_patterns,
                                                                  sequence)

                if nearby_PAS != 'NA':
                    nearby_PAS = '#'.join(nearby_PAS)
                    PAS_distance = '#'.join([str(di) for di in PAS_distance])

                number_supporting_reads = len(cls_reads)
                number_unique_supporting_reads = len(set(cls_reads))
                unique_reads_spread = math.sqrt(sum([(cls_center-cls_pos)**2 for
                                                     cls_pos in
                                                     cls_reads])/len(cls_reads))

                output_dict = {'chrm':chrm, 'beg': str(beg), 'end': str(end),
                           'utr_ID': feature_id, 'strand': strand,
                           'polyA_coordinate': str(cls_center),
                           'polyA_coordinate_strand': pA_coord_strand,
                           'annotated_polyA_distance':str(annotated_polyA_distance),
                           'nearby_PAS': nearby_PAS,
                           'PAS_distance': PAS_distance,
                           'PET_support': str(PET_support),
                           'cufflinks_support': str(cufflinks_support),
                           'polyA_average_composition': tail_info,
                           'number_supporting_reads': str(number_supporting_reads),
                           'number_unique_supporting_reads':\
                           str(number_unique_supporting_reads),
                           'unique_reads_spread':\
                           str(unique_reads_spread) .format('.2f')}

                output = [output_dict[keyw] for keyw in output_order]

                polyA_outfile.write('\t'.join(output) + '\n')

    polyA_outfile.close()

    return polyA_outpath

def make_directories(here, dirnames, DEBUGGING):
    """
    For each name in dirnames, return a list of paths to newly created
    directories in folder 'here'. Don't overwrite folders if they exist.
    """
    outdirs = []

    if DEBUGGING:
        # if DEBUGGING, make a DEBUGGING output directory
        debug_dir = os.path.join(here, 'DEBUGGING')
        if not os.path.exists(debug_dir):
            os.makedirs(debug_dir)

        # modify 'here' to poin to the debugging dir
        here = debug_dir


    for dirname in dirnames:

        this_dir = os.path.join(here, dirname)

        if not os.path.exists(this_dir):
            os.makedirs(this_dir)

        outdirs.append(this_dir)

    return outdirs

# Make a bedfile for the 'length' UTRs.
def parse_length(in_length, out_length, out_header):

    in_handle = open(in_length, 'rb')
    orig_head = in_handle.next() # remove the header before reading

    out_handle = open(out_length, 'wb')
    # print the header of the output file
    out_handle.write(out_header+'\n')

    for line in in_handle:
        (chrm, beg, end, extendby, strand, ID, eps_coord) = line.split()[:7]
        # Skip the utrs without coverage
        if eps_coord == 'NA':
            continue

        # Calculate the new 'end' (strand dependent)
        eps_coord = int(eps_coord)
        if strand == '+':
            end = str(int(beg) + eps_coord)
        if strand == '-':
            beg = str(int(beg) + eps_coord)

        out_handle.write('\t'.join([chrm, beg, end, ID, '0', strand])+'\n')

    out_handle.close()
    in_handle.close()

def parse_clusters(in_clusters, out_clusters, out_header):
    """
    Parse the polyA cluster file from output and produce a bedfile of all the
    polyA clusters. The name is the utr_id and the score is the nr of supporting
    reads.
    """
    infile = open(in_clusters, 'rb')
    in_header = infile.next()

    outfile = open(out_clusters, 'wb')
    outfile.write(out_header + '\n')
    for line in infile:
        (chrm, utr_beg, utr_end, utr_id, clr_nr, strand, cl_pos,
         supp_reads) = line.split()[:8]

        cl_beg = cl_pos
        cl_end = str(int(cl_beg)+1)
        outfile.write('\t'.join([chrm, cl_beg, cl_end, utr_id, supp_reads,
                                 strand]) + '\n')

    infile.close()
    outfile.close()


def make_bigwigs(settings, annotation, tempdir, output_dir, here, onlyBed):
    """
    Make bigwig files of the polyA reads and the normal reads of the datasets
    specified under [BIGWIG] in the UTR_SETTINGS file.

    Turn the read coverage and polyA read .bedfiles into bigWig files.
    These files can in turn be viewed on the UCSC genome browser.

    Finally print out a USCS custom track line for the bigWig file.

    Steps:

        1) Intersect with 3UTR (or whatever original bedfile was used)
        2) Do .bed -> .bedgraph
        3) Do .bedgraph -> bigWig
        4) Print USCS line

    As well, get a bedfile for the relative lengths:
        1) Parse lengths_datset in output
        2) Save to output-dir
    """

    # Define shortcut variables
    dsets = settings.bigwig_datasets # the datsets we process
    utrfile_path = annotation.utrfile_path
    savedir = settings.bigwig_savedir
    url = settings.bigwig_url

    # hg19 and bed->bigwig files
    hg19 = '/users/rg/jskancke/phdproject/3UTR/the_project/ext_files/hg19'
    bedGtoBigWig = '/users/rg/jskancke/programs/other/bedGraphToBigWig'

    for_wig = []
    for_length = []
    for_polyA = []

    print('Checking if files are available...\n')
    for dset in dsets:
        polyAfile = 'polyA_reads_'+ dset +'_processed_mapped.bed'
        readfile = 'reads_'+ dset +'.bed'
        lengthfile = 'length_' + dset
        clusterfile = 'polyA_' + dset

        polyA_read_path = os.path.join(tempdir, polyAfile)
        readpath = os.path.join(tempdir, readfile)
        lengthpath = os.path.join(output_dir, lengthfile)
        cluster_polyA_path = os.path.join(output_dir, clusterfile)

        # Check if the files are there; if so add them to list
        for bedfile in [polyA_read_path, readpath]:
            try:
                open(bedfile, 'rb')
                for_wig.append(bedfile)
            except:
                print('Not found or no access:\n{0}\nSkipping file...\n'\
                      .format(bedfile))

        # check length-file
        try:
            open(lengthpath, 'rb')
            for_length.append(lengthpath)
        except:
            print('Not found or no access:\n{0}\nSkipping file...\n'\
                  .format(lengthpath))

        # check polyA-cluster file
        try:
            open(cluster_polyA_path, 'rb')
            for_polyA.append(cluster_polyA_path)
        except:
            print('Not found or no access:\n{0}\nSkipping file...\n'\
                  .format(cluster_polyA_path))

    print('Done checking.\n')

    ######################## BedTools reads -> BigWig ##################
    # do the bedTools work to make BigWig files for the genome browser
    for dset in for_wig:

        (dirname, filename) = os.path.split(dset)

        # Adapt file names
        if filename.startswith('polyA'):
            shortname = '_'.join(filename.split('_')[:-2])
            co = '255,0,0' # red color
        if filename.startswith('reads'):
            shortname = os.path.splitext(filename)[0]
            co = '0,0,255' # blue color

        bedG_path = os.path.join(savedir, shortname + '.bedGraph')
        bigW_path = os.path.join(savedir, shortname + '.bigWig')

        if onlyBed:
            bed_path = os.path.join(savedir, shortname + '_coverage_.bed')


        dset_sort = dset+'_sorted'
        # Don't sort file if sorted file exists. DANGEROUS BUT FASTER.
        if not os.path.isfile(dset_sort):
            print('Sorting {0} ...'.format(dset))
            sort_cmd = ['sort', '-k', '1,1', dset]
            e = Popen(sort_cmd, bufsize=-1, stdout = open(dset_sort, 'wb'))
            e.wait()

        if os.path.isfile(bigW_path):
            print('')
            print('Found: {0}\nDelete it if you want to remake BigWig from source.'\
                  .format(bigW_path))
            print('')

        # Only make bigwig files if they don't exist. Delete to remake.
        else:
            cmd_intersect = ['intersectBed', '-a', dset_sort, '-b', utrfile_path]
            cmd_bedGraph = ['genomeCoverageBed', '-bg', '-i', 'stdin', '-g', hg19]
            cmd_bedGtoBW = [bedGtoBigWig, bedG_path, hg19, bigW_path]

            # of only bed don't make the other stuff
            if onlyBed:
                f = Popen(cmd_intersect, stdout = open(bed_path, 'wb'))
                continue

            f = Popen(cmd_intersect, stdout = PIPE)
            g = Popen(cmd_bedGraph, stdin = f.stdout, stdout = open(bedG_path, 'wb'))

            print('Running intersectBed + genomeCoverageBed on {0} ...'\
                  .format(dset_sort))

            g.wait() # wait for bedGraph to finish
            h = Popen(cmd_bedGtoBW)

            print('Running bedGraphToBigWig on {0} ...'.format(bedG_path))
            h.wait() # wait for bigWig to finish

        UCSC = 'track type=bigWig name="{0}" description="{0}" bigDataUrl={1} '\
        'color={2} visibility=2'\
                .format(shortname, os.path.join(url, shortname + '.bigWig'), co)

        print('Provide this USCS custom track line:\n')
        print UCSC
        print('')

    ############### Parse Length output ##########################
    for dset in for_length:

        (dirname, filename) = os.path.split(dset)

        shortname = 'epsilon_' + filename

        length_path = os.path.join(savedir, shortname)

        # Header of the bedfile
        header = 'track type=bed name="{0}" description="{0}" color=0,255,0'\
                .format(shortname, os.path.join(url, length_path))
        print('Parsing the file with epsilon-lengths ...')

        # parse the output length-file to make a bedfile for how long the 3UTRs
        # are given the read coverage
        parse_length(dset, length_path, header)

        print('')
        print('Upload this file to the genome browser:\n')
        print length_path
        print('')

        # Get the upstream/downstream stuff as well
        length_ud = 'udstream_' + shortname
        length_ud_path = os.path.join(savedir, length_ud+'.bedGraph')
        header = 'track type=bed name="{0}" description="{0}" color=0,255,0'\
                .format(length_ud)
        length_udstream_covr(dset, length_ud_path, header)

        # Get the location of the PAS and their distances.
        length_PAS = 'PAS_distance_' + shortname
        length_PAS_path = os.path.join(savedir, length_PAS+'.bedGraph')
        header = 'track type=bed name="{0}" description="{0}" color=158,35,135'\
                .format(length_PAS)

        pas_dist_bed_length(dset, length_PAS_path, header)


    ############### Parse the polyA output ########################
    for dset in for_polyA:
        (dirname, filename) = os.path.split(dset)

        shortname = 'clusters_' + filename

        cluster_path = os.path.join(savedir, shortname)

        header = 'track type=bed name="{0}" description="{0}" color=0,255,255'\
                .format(shortname, os.path.join(url, cluster_path))
        print('Parsing the file with polyA-clusters ...')

        parse_clusters(dset, cluster_path, header)

        print('')
        print('Upload this file to the genome browser:\n')
        print cluster_path
        print('')

        # Get the upstream, downstraem coverage 
        cluster_ud = 'udstream_' + shortname
        cluster_ud_path = os.path.join(savedir, cluster_ud+'.bedGraph')
        header = 'track type=bed name="{0}" description="{0}" color=0,255,255'\
                .format(cluster_ud)
        cluster_udstream_covr(dset, cluster_ud_path, header)

        # Get the location of the PAS and their distances.
        cluster_PAS = 'PAS_distance_' + shortname
        cluster_PAS_path = os.path.join(savedir, cluster_PAS+'.bedGraph')
        header = 'track type=bed name="{0}" description="{0}" color=128,35,175'\
                .format(cluster_PAS)

        pas_dist_bed_clusters(dset, cluster_PAS_path, header)


def length_udstream_covr(in_length, out_length, out_header):
    infile = open(in_length, 'rb')
    orig_head = infile.next() # remove the header before reading

    outfile = open(out_length, 'wb')
    # print the header of the output file
    outfile.write(out_header+'\n')

    for line in infile:
        (chrm, beg, end, ext, strand, ID, eps_coord, eps_rz, eps_ds_covr,
         eps_us_covr) = line.split()[:10]
        # Skip the utrs without coverage
        if eps_coord == 'NA':
            continue
        eps_coord = int(beg) + int(eps_coord)
        ds_covr = float(eps_ds_covr)
        us_covr = float(eps_us_covr)
        # Write 50 lines upstream/downstream (depending on strand) in bedgraph
        # format for easy viewing on the browser

        # Strand differene: what is ustream dn what is dstream
        if strand == '+':
            beg = eps_coord - 50
            end = eps_coord
            outfile.write('\t'.join([chrm, str(beg), str(end), str(us_covr)])+'\n')

            beg = eps_coord
            end = eps_coord + 50
            outfile.write('\t'.join([chrm, str(beg), str(end), str(ds_covr)])+'\n')

        if strand == '-':
            beg = eps_coord - 50
            end = eps_coord
            outfile.write('\t'.join([chrm, str(beg), str(end), str(ds_covr)])+'\n')

            beg = eps_coord
            end = eps_coord + 50
            outfile.write('\t'.join([chrm, str(beg), str(end), str(us_covr)])+'\n')

    outfile.close()
    infile.close()

def cluster_udstream_covr(in_cluster, out_cluster, out_header):
    infile = open(in_cluster, 'rb')
    orig_head = infile.next() # remove the header before reading

    outfile = open(out_cluster, 'wb')
    # print the header of the output file
    outfile.write(out_header+'\n')

    for line in infile:
        (chrm, beg, end, ID, pA_nr, strand, cl_coord, supp_reads, ds_covr,
         us_covr) = line.split()[:10]

        # YOU ARE WRITING THE POLYA CLUSTER DOWNSTREAM UPSTREAM
        ds_covr = float(ds_covr)
        us_covr = float(us_covr)
        cl_coord = int(cl_coord)

        # Write 50 lines upstream/downstream (depending on strand) in bedgraph
        # format for easy viewing on the browser

        # Strand differene: what is ustream dn what is dstream
        if strand == '+':
            beg = cl_coord - 50
            end = cl_coord
            outfile.write('\t'.join([chrm, str(beg), str(end), str(us_covr)])+'\n')

            beg = cl_coord
            end = cl_coord + 50
            outfile.write('\t'.join([chrm, str(beg), str(end), str(ds_covr)])+'\n')

        if strand == '-':
            beg = cl_coord - 50
            end = cl_coord
            outfile.write('\t'.join([chrm, str(beg), str(end), str(ds_covr)])+'\n')

            beg = cl_coord
            end = cl_coord + 50
            outfile.write('\t'.join([chrm, str(beg), str(end), str(us_covr)])+'\n')

    outfile.close()
    infile.close()


def pas_dist_bed_clusters(in_PAS, out_PAS, header):
    """
    Write bedfile with positions and distances of PASes to their clusters so you
    can check the result in the genome browser.
    """
    infile = open(in_PAS, 'rb')
    in_header = infile.next()

    outfile = open(out_PAS, 'wb')
    outfile.write(header + '\n')

    for line in infile:
        (chrm, beg, end, d, d, strand, cl_coord) = line.split('\t')[:7]
        (cl_PAS, cl_PAS_dist, rpkm) = line.split('\t')[-3:]
        cl_PAS = cl_PAS.split('#')
        cl_PAS_dist = cl_PAS_dist.split('#')

        for (pas, dist) in zip(cl_PAS, cl_PAS_dist):
            if pas != 'NA':
                # get the beg and end of the PAS
                if strand == '+':
                    pas_beg = str(int(cl_coord) - int(dist)-7)
                    pas_end = str(int(cl_coord) - int(dist)-1)

                if strand == '-':
                    pas_beg = str(int(cl_coord) + int(dist)-1)
                    pas_end = str(int(cl_coord) + int(dist)+5)

                pas_and_dist = pas+'_'+dist

                outfile.write('\t'.join([chrm, pas_beg, pas_end, pas_and_dist])+'\n')

    outfile.close()

def pas_dist_bed_length(in_PAS, out_PAS, header):
    """
    Write bedfile with positions and distances of PASes to their clusters so you
    can check the result in the genome browser.
    """
    infile = open(in_PAS, 'rb')
    infile.next() # skip header

    outfile = open(out_PAS, 'wb')
    outfile.write(header + '\n')

    for line in infile:
        (chrm, beg, end, d, strand, d, eps_coord) = line.split('\t')[:7]
        (eps_PAS, eps_PAS_dist, rpkm, avrg_covrg) = line.split('\t')[-4:]
        eps_PAS = eps_PAS.split('#')
        eps_PAS_dist = eps_PAS_dist.split('#')

        for (pas, dist) in zip(eps_PAS, eps_PAS_dist):
            if pas != 'NA':
                # get the beg and end of the PAS
                if strand == '+':
                    pas_beg = str(int(eps_coord)+int(beg) - int(dist)-7)
                    pas_end = str(int(eps_coord)+int(beg) - int(dist)-1)

                if strand == '-':
                    pas_beg = str(int(eps_coord)+int(beg) + int(dist)-1)
                    pas_end = str(int(eps_coord)+int(beg) + int(dist)+5)

                pas_and_dist = pas+'_'+dist

                outfile.write('\t'.join([chrm, pas_beg, pas_end,
                                         pas_and_dist])+'\n')

    outfile.close()

def main():
    """
    This method is called if script is run as __main__.
    """

    # Set debugging mode. This affects the setting that are in the debugging
    # function (called below). It also affects the 'temp' and 'output'
    # directories, respectively.

    #DEBUGGING = True # warning... some stuff wasnt updated here
    DEBUGGING = False

    # with this option, always remake the bedfiles
    rerun_annotation_parser = False
    #rerun_annotation_parser = True

    # The path to the directory the script is located in
    here = os.path.dirname(os.path.realpath(__file__))

    # Make directories needed by downstream code
    # If debugging, make debugging output
    dirnames = ['temp_files', 'source_bedfiles', 'output']
    (tempdir, beddir, output_dir) = make_directories(here, dirnames, DEBUGGING)

    # Location of settings file
    settings_file = os.path.join(here, 'UTR_SETTINGS')

    # Create the settings object from the settings file
    settings = Settings(*read_settings(settings_file))

    # You can chose to not simulate. Only purpose is to make bigwigs.
    #simulate = False
    simulate = True

    # This option should be set only in case of debugging. It makes sure you
    # just run chromosome 1 and only extract a tiny fraction of the total reads.
    if DEBUGGING:
        settings.DEBUGGING()

    # The program reads a lot of information from the annotation. The annotation
    # object will hold this information (file-paths and datastructures).
    print('Reading settings ...\n')
    annotation = Annotation(settings.annotation_path,
                            settings.annotation_format,
                            settings.annotated_polyA_sites)

    # loop through all regions supplied
    for region_file in settings.region_files:
        # update the region bedfile for each run

        piperunner(settings, annotation, simulate, DEBUGGING, beddir, tempdir,
                   output_dir, rerun_annotation_parser, here, region_file)

def piperunner(settings, annotation, simulate, DEBUGGING, beddir, tempdir,
               output_dir, rerun_annotation_parser, here, region_file):

    # Check if 3UTRfile has been made or provided; if not, get it from annotation
    annotation.utrfile_path = get_utr_path(settings, beddir,
                                           rerun_annotation_parser, region_file)

    # Get dictionary with (chrm, beg, end, strand) values for each feature
    # (feature = 3utr-exonic, intergenic, etc). Note that the values may be
    # extended if so desired in the program code.
    print('Making data structures ...\n')
    annotation.feature_coords = annotation.get_utrdict()

    # You intersect the annotated polyA files with the region file provided.
    # Put the intersected annotated polyA sites in a dictionary.
    print('Making data structures for annotated poly(A) sites ...\n')
    annotation.a_polyA_sites_dict = annotation.get_annot_polyA_dict(settings.chr1)

    # AD hoc PET -- should be included in the settings file :(
    annotation.PETdict = annotation.get_PET_dict()

    # :/ more AD HOC! This time cufflinks
    annotation.cufflinksDict = annotation.get_cuff_dict()

    # make a similar dictionary but for pet-reads

    # If this is true, the poly(A) reads will be saved after getting them. That
    # will save all that reading. They will only be saved if you run with no
    # restriction in reads.
    polyA_cache = True
    #polyA_cache = False # XX fix this sometime

    # Extract the name of the bed-region you are checking for poly(A) reads
    # you might have to extract it differently, because 'intergenic' doesn't
    # have any exon or intron part to it.
    region = os.path.basename(annotation.utrfile_path).split('_')[0]
    # ad hoc hack for intergenic region

    ##################################################################
    if simulate:
        # Create a pool of processes; one dataset will take up one process.
        my_pool = Pool(processes = settings.max_cores)
        results = []

        # Apply all datasets to the pool
        t1 = time.time()

        # dset_id and dset_reads are as given in UTR_SETTINGS
        for dset_id, dset_reads in settings.datasets.items():

            #update dset_id to contain region information (3UTR, CSDEXON, etc)
            dset_id = dset_id + '_' + region

            # The arguments needed for the pipeline
            arguments = (dset_id, dset_reads, tempdir, output_dir, settings,
                         annotation, DEBUGGING, polyA_cache, here)

            ###### FOR DEBUGGING ######
            #akk = pipeline(*arguments)
            ###########################

            result = my_pool.apply_async(pipeline, arguments)
            results.append(result)

        #debug()
        my_pool.close()
        my_pool.join()

        # we don't return anything, but get results anyway
        [result.get() for result in results]

        # Print the total elapsed time
        print('Total elapsed time: {0}\n'.format(time.time()-t1))

    ###################################################################

    # if you only want the bed coverage, set it here.
    onlyBed = True
    # if set, make bigwig files
    if settings.bigwig:
        make_bigwigs(settings, annotation, tempdir, output_dir, here, onlyBed)

if __name__ == '__main__':
    main()

# NOTES:
    # Time for transcription < 10 minutes
    # Time for diffusion in nucleoplasm before transport to cytoplasm ~10 min
    # Half-life in cytoplasm: median 7h (average 12h?)

# PAPER IDEAS:
    # 1) Reproduce everything they have done with ESTs. This shows that detailed
    # sequencing now gives the same results as all the ESTs combined! :)
    # 2) Compare with the nature paper with 36bp reads. Even if they have many
    # more unmapped reads, the read length is critical to get good information.
    # Run the whole pipelien with _JUST_THE_EXTENSIOSN! you can make an
    # extension.bed file and mark it with the gene you extend from. extend until
    # you meet another annotated object, and at most a certain distance.
    # 3) Consult Hagen about what he would expect from the different
    # compartments
    # 4) The postoc's speech was that methylation downstream a poly(A) site can
    # contribute to polyadenylation, thuogh Hagen injected that it could
    # contribute to the slowing and subsequent release of the polymerase
    # 5) Do statistics on the number of times polyA reads fall in the opposite
    # strand (for all clusters and for clusters with more than 1 read)
    # 6) Give the absolute number of poly(A) reads in the different
    # compartments. Seems like there are a lot more poly(A) reads in the
    # cytoplasm than in the nucleus
    # 7) Does the 3UTR resolution benefit from the compartment data? Are there
    # some things in whole cell that even out the a difference in cytosol and
    # nucleus?

